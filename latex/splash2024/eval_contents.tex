\section{Evaluation}

We consider the following research questions for our evaluation:

\begin{itemize}
\item \textbf{RQ 1}: What are the characteristics of natural repairs? (Levenshtein edit distance, distance between edits etc)
\item \textbf{RQ 2}: How do we compare with SoTA approaches? (e.g., Seq2Parse, BIFI, OrdinalFix, et al.)
\item \textbf{RQ 3}: Ablation study over design choices? (search vs. sampling, timing cutoffs, ngram models, etc.)
\end{itemize}

For our evaluation, we use the StackOverflow dataset from~\cite{hindle2012naturalness}. We preprocess the dataset to lexicalize both the broken and fixed code snippets, then filter the dataset by length and edit distance, in which all Python snippets whose broken form is fewer than 80 lexical tokens and whose human fix is under four Levenshtein edits is retained.

\subsection{Dataset}

To evaluate our model, we primarily use 5,600 pairs of (broken, fixed) Python code snippets from Wong et al.'s StackOverflow dataset~\cite{wong2019syntax} shorter than 40 lexical tokens, whose patch sizes are shorter than four lexical tokens ($|\Sigma| = 50, |\err{\sigma}| \leq 40, \Delta(\err{\sigma}, \ell) < 4$), and adapt the Python grammar from SeqParse.

In the first set of experiments, we uniformly sample without replacement from the Levenshtein edit ball using a LFSR over $\mathbb{Z}_2^m$, and measure the precision@k of our repair procedure against human repairs of varying edit distances and latency cutoffs. This provides a baseline for the relative density of the admissible set, and an upper bound on the latency of attaining a given precision.

In the second set of experiments, we use an adaptive sampler that stochastically resamples edits using a Dirichlet process. Repairs are scored and placed into a ranked buffer, from which new edits are resampled with frequency relative to their perplexity, and additive noise is introduced. The adaptive sampler is described in further detail in \S\ref{sec:adaptive}.

To train the scoring function, we use a length-5 variable-order Markov (VOM) chain implemented using a count-min sketch based on Apache Datasketches~\cite{apache2022datasketches}. Training on 55 million StackOverflow tokens from the BIFI~\cite{yasunaga2021break} dataset took roughly 10 minutes, after which calculating perplexity is nearly instantaneous. Sequences are scored using negative log likelihood with Laplace smoothing and our evaluation measures the precision@\{1, 5, 10, All\} for samples at varying latency cutoffs.

Both sets of experiments were conducted on 40 Intel Skylake cores running at 2.4 GHz, with 16 GB of RAM, running bytecode compiled for JVM 17.0.2. We measure the precision using abstract lexical matching, following the Seq2Parse~\cite{sakkas2022seq2parse} evaluation, and give a baseline for their approach on the same dataset.

The StackOverflow dataset is comprised of 500k Python code snippets, each of which has been annotated with a human repair. We depict the normalized edit loations relative to the snippet length below.

\begin{figure}[h!]
  \begin{tikzpicture}
    \begin{axis}[
    ybar,
    bar width=15pt,
    xlabel={Beginning of snippet $\longleftrightarrow$ End of snippet},
    ylabel={Frequency},
    title={Normalized edit locations},
    ymin=0,
    ymax=35,
    xtick=data,
    xticklabels={10\%,20\%,30\%,40\%,50\%,60\%,70\%,80\%,90\%,100\%},
    ymajorgrids=true,
    grid style=dashed,
    width=\textwidth,
    height=0.3\textwidth
    ]

    \addplot table {
      X Y
      10 11.6539
      20 5.7252
      30 6.2087
      40 5.9542
      50 5.5980
      60 7.9389
      70 7.0738
      80 6.9466
      90 12.4173
      100 30.4835
    };
    \end{axis}
  \end{tikzpicture}
\end{figure}

\noindent Likewise, we can plot the number of tokens between edits within each patch:

\begin{figure}[h!]
  \begin{tikzpicture}
    \begin{axis}[
      ybar,
      bar width=15pt,
      title={Intra-patch edit distance},
      xlabel={Caret distance},
      ylabel={Frequency},
      xtick=data,
      ymajorgrids=true,
      grid style=dashed,
      xticklabels={1,2,3,4,5,6,7,8,9,10+},
      width=\textwidth,
      height=0.3\textwidth
    ]

      \addplot table {
        X Y
        1 40.66
        2 15.00
        3 5.80
        4 4.86
        5 4.26
        6 2.98
        7 2.05
        8 2.73
        9 1.62
        10 13.64
      };
    \end{axis}
  \end{tikzpicture}
\end{figure}

For our first experiment, we run the sampler until the human repair is detected, then measure the number of samples required to draw the exact human repair across varying Levenshtein radii.

\begin{figure}[h!]
%  \input{sample_efficiency}
  \caption{Sample efficiency of LBH sampler at varying Levenshtein radii.}\label{fig:sample_efficiency}
\end{figure}

Next, measure the precision at various ranking cutoffs for varying wall-clock timeouts. Here, P@\{k=1, 5, 10, All\} indicates the percentage of syntax errors with a human repair of $\Delta=\{1, 2, 3, 4\}$ edits found in $\leq p$ seconds that were matched within the top-k results, using an n-gram likelihood model.

\begin{figure}[h!]
%    \resizebox{.19\textwidth}{!}{\input{bar_hillel_repair.tex}}
  \resizebox{.24\textwidth}{!}{\input{bar_hillel_repair_1}}
  \resizebox{.24\textwidth}{!}{\input{bar_hillel_repair_2}}
  \resizebox{.24\textwidth}{!}{\input{bar_hillel_repair_3}}
  \resizebox{.24\textwidth}{!}{\input{bar_hillel_repair_4}}
%    \resizebox{.24\textwidth}{!}{\input{bar_hillel_repair_5}}
%\resizebox{.3\textwidth}{!}{\input{repair1_plot.tex}}
%\resizebox{.307\textwidth}{!}{\input{repair2_plot.tex}}
  \caption{Human repair benchmark. Note the y-axis across different edit distance plots has varying ranges.}\label{fig:human}
\end{figure}