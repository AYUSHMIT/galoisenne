%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[sigplan,10pt,review,anonymous]{acmart}
%\settopmatter{printfolios=false,printccs=false,printacmref=false}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
%\documentclass[sigplan,nonacm]{acmart}
\documentclass[sigplan,review,anonymous,acmsmall]{acmart}\settopmatter{printfolios=false,printccs=false,printacmref=false}


%% Conference information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
\acmConference[SPLASH'24]{ACM SIGPLAN conference on Systems, Programming, Languages, and Applications: Software for Humanity}{October 22-27, 2024}{Pasadena, California, United States}
%\acmYear{2018}
%\acmISBN{} % \acmISBN{978-x-xxxx-xxxx-x/YY/MM}
%\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
%\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2018}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{acmart}

\input{preamble}
\begin{document}
%
  \title{Syntax Repair as Language Intersection}
  %
  \begin{abstract}
    We introduce a new technique for correcting syntax errors in arbitrary context-free languages. Our work comes from the observation that syntax errors with a small repair typically have very few unique small repairs, which can usually be enumerated up to a small edit distance then ranked within a short amount of time. Furthermore, we place a heavy emphasis on precision: the enumerated set must contain every possible repair within a few edits and no invalid repairs. To do so, we reduce CFL recognition onto Boolean tensor completion, then model error correction as a language intersection problem between a Levenshtein automaton and a context-free grammar. To decode the solutions, we then sample trees without replacement from the intersection grammar, which yields valid repairs within a certain Levenshtein distance. Finally, we rank all repairs discovered within 60 seconds by a Markov chain.
    \keywords{Error correction \and CFL reachability \and Langauge games.}
  \end{abstract}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
%  \author{Breandan Considine\inst{1} \and
%  Jin Guo\inst{1}\and
%  Xujie Si\inst{2}}
%
%  \authorrunning{Considine et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
%  \institute{McGill University, Montr\'eal, QC H2R 2Z4, Canada\\
%  \email{\{breandan.considine@mail, jguo@cs\}.mcgill.ca}\and
%  University of Toronto, Toronto, ON, M5S 1A1 Canada\\
%  \email{six@utoronto.ca}}
%
  \maketitle              % typeset the header of the contribution


  \section{Introduction}

  Syntax repair is the problem of modifying an invalid sentence so it conforms to some grammar. Prior work has been devoted to fixing syntax errors using handcrafted heuristics. This work features a variety of approaches including rule-based systems and statistical language models. However, these techniques are often brittle, and are susceptible to misgeneralization. In a prior paper published in SPLASH, the authors sample production rules from an error correcting grammar. While theoretically sound, this technique is incomplete, i.e., not guaranteed to sample all edits within a certain Levenshtein distance, and no more. In this paper, we demonsrate it is possible to attain a significant advantage by synthesizing and scoring all repairs within a certain Levenshtein distance. Not only does this technique guarantee perfect generalization, but also helps with precision.

  We take a first-principles approach making no assumptions about the sentence or grammar and focuses on correctness and end-to-end latency. Our technique is simple:

  \begin{enumerate}
    \item We first reduce the problem of CFL recognition to Boolean tensor completion, then use that to compute the Parikh image of the CFL. This follows from a straightforward extension of the Chomsky-Sch\"utzenberger enumeration theorem.
    \item We then model syntax correction as a language intersection problem between a Levenshtein automaton and a context-free grammar, which we explicitly materialize using a specialized version of the Bar-Hillel construction to Levenshtein intersections. This greatly reduces the number of generated productions.
    \item To decode the members from the intersection grammar, we sample trees without replacement by constructing a bijection between syntax trees and the integers, then sampling integers uniformly without replacement from a finite range. This yields concrete repairs within a certain Levenshtein distance.
    \item Finally, we rerank all repairs found before a fixed timeout by n-gram perplexity.
  \end{enumerate}

  Though simple, this technique outperforms SoTA syntax repair techniques. Its efficacy owes to the fact it does not sample edits or nor productions, but unique, fully formed repairs within a certain Levenshtein distance. It is sound and complete up to a Levenshtein bound - i.e., it will find all repairs within an arbitrary Levenshtein distance, and no more. Often the language of small repairs is surprisingly small compared with the language of all possible edits, enabling us to efficiently synthesize and score every possible solution. This offers a significant advantage over memoryless sampling techniques.

  \pagebreak

  \section{Example}

  Consider the following Python snippet, which contains a small syntax error:\\

  \noindent\texttt{def prepend(i, k, L=[]) n and [prepend(i - 1, k, [b] + L) for b in range(k)]}\\

  We can fix it by inserting a colon after the function definition, yielding:\\

  \noindent\texttt{def prepend(i, k, L=[])\hlgreen{:} n and [prepend(i - 1, k, [b] + L) for b in range(k)]}\\

  A careful observer will note that there is only one way to repair this Python snippet by making a single edit. In fact, many programming languages share this curious property: syntax errors with a small repair have few uniquely small repairs. Valid sentences corrupted by a few small errors rarely have many small corrections. We call such sentences \textit{metastable}, since they are relatively stable to small perturbations, as likely to be incurred by a careless typist or novice programmer.
%  Consider the following Kotlin snippet:\\
%
%  \texttt{fun main() = try \{ fetch() \} except(e: Exception) \{ handle(e) \}}\\
%
%  \noindent Again, there are thousands of possible single-token edits, only one of which is a valid repair:\\
%
%  \texttt{fun main() = try \{ fetch() \} \hlorange{catch}(e: Exception) \{ handle(e) \}}\\

  Let us consider a slightly more ambiguous error: \texttt{v = df.iloc(5:, 2:)}. Assuming an alphabet of just one hundred lexical tokens, this tiny statement has millions of possible two-token edits, yet only six of those possibilities are accepted by the Python parser:

%\setlength{\columnsep}{-10pt}
%\setlength{\columnseprule}{-10pt}
%\noindent\begin{multicols}{3}
%  \begin{enumerate}
%    \item\texttt{v = df.iloc(5\hlred{:}, 2\hlorange{,})}\\
%    \item\texttt{v = df.iloc(5\hlgreen{[}:, 2:\hlgreen{]})}\\
%    \item\texttt{v = df.iloc\hlorange{[}5:, 2:\hlorange{]}}\\
%    \item\texttt{v = df.iloc(5\hlorange{)}, 2\hlorange{(})}\\
%    \item\texttt{v = df.iloc(5\hlred{:}, 2\hlred{:})}\\
%    \item\texttt{v = df.iloc(5\hlgreen{[}:, 2\hlorange{]})}
%  \end{enumerate}
%\end{multicols}
  \begin{figure}[h!]
    \noindent\begin{tabular}{@{}l@{\hspace{10pt}}l@{\hspace{10pt}}l@{}}
    (1) \texttt{v = df.iloc(5\hlred{:}, 2\hlorange{,})} & (3) \texttt{v = df.iloc(5\hlgreen{[}:, 2:\hlgreen{]})} & (5) \texttt{v = df.iloc\hlorange{[}5:, 2:\hlorange{]}} \\\\
    (2) \texttt{v = df.iloc(5\hlorange{)}, 2\hlorange{(})} & (4) \texttt{v = df.iloc(5\hlred{:}, 2\hlred{:})} & (6) \texttt{v = df.iloc(5\hlgreen{[}:, 2\hlorange{]})} \\
    \end{tabular}
  \end{figure}

  With some typing information, we could easily narrow the results, but even in the absence of semantic constraints, one can probably rule out (2, 3, 6) given that \texttt{5[} and \texttt{2(} are rare bigrams in Python, and knowing \texttt{df.iloc} is often followed by \texttt{[}, determine (5) is most natural. This is the key insight behind our approach: we can usually locate the intended fix by exhaustively searching small repairs. As the set of small repairs is itself often small, if only we had some procedure to distinguish valid from invalid patches, the resulting solutions could be simply ranked by naturalness.

  The trouble is that any such procedure must be highly sample-efficient. We cannot afford to sample the universe of possible d-token edits, then reject invalid ones -- assuming it takes just 10ms to generate and check each sample, (1-6) could take 24+ hours to find. The hardness of brute-force search grows superpolynomially with edit distance, sentence length and alphabet size. We need a more efficient procedure for sampling all and only small valid repairs.

  \section{Problem}

  We can model syntax repair as a language intersection problem between a context-free language (CFL) and a regular language.

  \begin{definition}[Bounded Levenshtein-CFL reachability]
    Given a CFL $\ell$ and an invalid string $\err{\sigma}: \ell^\complement$, the BCFLR problem is to find every valid string reachable within $d$ edits of $\err{\sigma}$, i.e., letting $\Delta$ be the Levenshtein metric and $L(\err\sigma, d) \coloneqq \{\sigma \mid \Delta(\err{\sigma}, \sigma) \leq d\}$, we seek to find $A = L(\err\sigma, d) \cap \ell$.
  \end{definition}

%  To solve this problem, it is convenient to first consider intersections with a finite-length string with holes, then turn our attention back to BCFLR.
%

  As the admissible set $A$ is typically under-determined, we want a procedure which prioritizes natural and valid repairs over unnatural but valid repairs:

  \begin{definition}[Ranked repair]\label{def:ranked-repair}
    Given a finite language $A = L(\err\sigma, d) \cap \ell$ and a probabilistic language model $\text{P}_\theta: \Sigma^* \rightarrow [0, 1] \subset \mathbb{R}$, the ranked repair problem is to find the top-$k$ maximum likelihood repairs under the language model. That is,
    \begin{equation}
      R(A, P_\theta) \coloneqq \argmax_{\{\bm{\sigma} \mid \bm{\sigma} \subseteq A, |\bm{\sigma}| \leq k\}} \sum_{\sigma \in \bm{\sigma}}\text{P}_\theta(\sigma\mid\err\sigma)
    \end{equation}
    % On average, across all $G, \sigma$ $\hat{R}$ should approximate $R$.
%    We want a procedure $\hat{R}$, minimizing $\mathbb{E}_{G, \sigma}\big[D_{\text{KL}}(\hat{R} \parallel R)\big]$ and wallclock runtime.
  \end{definition}

  The problem of ranked repair is typically solved by learning a distribution over strings, however this approach can be sample-inefficient and generalize poorly to new languages. Representing the problem as a distribution over $\Sigma^*$ forces the language model to jointly learn syntax and stylometry. As our work demonstrates, this problem can be factorized into a bilevel objective: first retrieval, then ranking. By ensuring retrieval is sufficiently precise and exhaustive, maximizing likelihood over the admissible set can be achieved with a much simpler, syntax-oblivious language model.

  Even with an extremely efficient approximate sampler for $\sigma \sim \ell_\cap$, due to the size of $\ell$ and $L(\err\sigma, d)$, it would be intractable to sample either $\ell$ or $L(\err\sigma, d)$, then reject invalid ($\sigma \notin \ell$) or unreachable ($\sigma \notin L(\err\sigma, d)$) edits, and completely out of the question to sample $\sigma \sim \Sigma^*$ as do many large language models. Instead, we will explicitly construct a grammar generating $\ell \cap L(\err\sigma, d)$, sample from it, then rerank all repairs after a fixed timeout. So long as $|\ell_\cap|$ is sufficiently small and recognizes all and only small repairs, our sampler is sure to retrieve the most natural repair and terminate quickly.

  \section{Method}

  \begin{wrapfigure}{r}{0.45\textwidth}
%\begin{figure}[h!]
    \vspace{-1.2cm}
    \begin{center}
      \resizebox{0.45\textwidth}{!}{
        \begin{tikzpicture}[node distance=2cm]
          \node (start) [startstop] {Start};
          \node (pro1) [process, below of=start, yshift=-0.5cm] {$G_\cap \leftarrow G\cap\Delta(\err\sigma, d)$};
          \node (pcfg) [io, left of=pro1, xshift=-3cm] {[P]CFG};
          \node (lnfa) [io, right of=pro1, xshift=3cm] {L-WFA};

          \node (code) [io, right of=start,xshift=3cm] {Code};
          \node (synt) [io, left of=start,xshift=-3cm] {Syntax};

          \node (dec1) [decision, below of=pro1, yshift=-0.5cm] {$[G_{\cap} = \varnothing]$};

          \node (pro2b) [process, right of=dec1, xshift=3cm] {Increase radius, $d$};

          \node (dec2) [decision, below of=dec1, yshift=-1cm] {$|\mathcal{L}(G_\cap)|$};

          \node (samp1) [process, left of=dec2, xshift=-3cm] {Enumerate $\sigma' \in \mathcal{L}(G_\cap)$};
          \node (samp2) [process, right of=dec2, xshift=3cm] {Sample $\sigma' \sim P(G_\cap)$};

          \node (rank) [process, below of=dec2, yshift=-2.5cm] {Rerank by $L_\theta(\sigma')$};
          \node (vlmc) [io, above of=rank, yshift=-0.1cm] {VLMC};

%  \node (out1) [io, below of=pro2a] {Output};
          \node (stop) [startstop, below of=rank] {Done};

%  \draw [arrow] (dec0) -- node[anchor=east] {no} (pro1);

          \draw [->,thick] (0, 1.3) -- (start);

          \draw [arrow] (start) -- (code);
          \draw [arrow] (start) -- (synt);
          \draw [arrow] (code) -- (lnfa);
          \draw [arrow] (synt) -- (pcfg);
          \draw [arrow] (lnfa) -- (pro1);
          \draw [arrow] (pcfg) -- (pro1);

%  \draw [arrow] (in1) -- (pro1);
          \draw [arrow] (pro1) -- (dec1);
          \draw [arrow] (dec1) -- node[anchor=south] {yes} (pro2b);
          \draw [arrow] (dec1) -- node[anchor=east] {no} (dec2);
          \draw [arrow] (pro2b) -- (lnfa);
          \draw [arrow] (dec2) -- node[anchor=south] {small} (samp1);
          \draw [arrow] (dec2) -- node[anchor=south] {large} (samp2);

          \draw [arrow] (vlmc) -- (rank);
          \draw [arrow] (samp1) |- (rank);
          \draw [arrow] (samp2) |- (rank);
%  \draw [arrow] (pro2a) -- (out1);
          \draw [arrow] (rank) -- (stop);
%          \draw [arrow] (dec2) -- node[anchor=east] {1} (stop);

        \end{tikzpicture}
      }
    \end{center}
    \vspace{-0.1cm}
    \caption{Flowchart of our proposed method.}\label{fig:flowchart}
    \vspace{-1cm}
  \end{wrapfigure}

  The syntax of most programming languages is context-free. Our proposed method is simple. We construct a context-free grammar representing the intersection between the language syntax and an automaton recognizing the Levenshtein ball of a given radius. Since CFLs are closed under intersection with regular languages, this is admissible. Three outcomes are possible:

  \begin{enumerate}
    \item $G_\cap$ is empty, in which case there is no repair within the given radius. In this case, we simply increase the radius and try again.
    \item $\mathcal{L}(G_\cap)$ is small, in which case we enumerate all possible repairs. Enumeration is tractable for $\sim 80\%$ of the dataset in $\leq 90$s.
    \item $\mathcal{L}(G_\cap)$ is too large to enumerate, so we sample from the intersection grammar $G_\cap$. Sampling is necessary for $\sim20\%$ of the dataset.
  \end{enumerate}

  As long as we have done our job correctly, the intersection language should contain every repair within a certain Levenshtein distance, and no invalid repairs. This procedure is depicted in Fig.~\ref{fig:flowchart}. In the following section, we will describe how we construct the intersecection grammar (\S~\ref{sec:lev_nfa}, \ref{sec:lev_bh}), then, provide an explicit technique for extracting all repairs contained within it (\S~\ref{sec:ptree}). Finally, we use an n-gram model to rank and return the top-k results by likelihood (\S~\ref{sec:ranking}).

  \subsection{Preliminaries}

  Recall that a CFG, $\mathcal{G} = \langle \Sigma, V, P, S\rangle$, is a quadruple consisting of terminals $(\Sigma)$, nonterminals $(V)$, productions $(P\colon V \rightarrow (V \mid \Sigma)^*)$, and a start symbol, $(S)$. Every CFG is reducible to \textit{Chomsky Normal Form}, $P'\colon V \rightarrow (V^2 \mid \Sigma)$, in which every production takes one of two forms, either $w \rightarrow xz$, or $w \rightarrow t$, where $w, x, z: V$ and $t: \Sigma$. For example:\vspace{-3pt}

  \begin{table}[H]
    \begin{tabular}{llll}
      $G\coloneqq\big\{\;S \rightarrow S\:S \mid (\:S\:) \mid (\:)\;\big\} \Longrightarrow \big\{\;S\rightarrow Q\:R \mid S\:S \mid L\:R,$ & $R \rightarrow\:),$ & $L \rightarrow (,$ & $Q\rightarrow L\:S\;\big\}$
    \end{tabular}
  \end{table}\vspace{-8pt}

  Likewise, a finite state automaton is a quintuple $\mathcal{A} = \langle Q, \Sigma, \delta, I, F\rangle$, where $Q$ is a finite set of states, $\Sigma$ is a finite alphabet, $\delta \subseteq Q \times \Sigma \times Q$ is the transition function, and $I, F \subseteq Q$ are the set of initial and final states, respectively. We will use this notation in the following sections.

  \pagebreak\subsection{The Nominal Levenshtein Automaton}\label{sec:lev_nfa}

  \begin{wrapfigure}{r}{0.5\textwidth}
    \vspace{-0.3cm}
    \begin{center}
      \input{nfa_cfg.tex}
    \end{center}
    \caption{NFA recognizing Levenshtein $\Delta(\sigma: \Sigma^5, 3)$.}\label{fig:lev_nfa}
    \vspace{-0.5cm}
  \end{wrapfigure}

  Levenshtein edits are recognized by an automaton known as the Levenshtein automaton. As the original construction defined by Schultz and Mihov~\cite{schulz2002fast} contains cycles and $\varepsilon$-transitions, we propose a variant which is $\varepsilon$-free and acyclic. Furthermore, we adpot a nominal form which supports infinite alphabets and considerably simplifies the language intersection. Illustrated in Fig.~\ref{fig:lev_nfa} is an example of a small Levenshtein automaton recognizing $\Delta(\sigma: \Sigma^5, 3)$. Unlabeled arcs accept any terminal from the alphabet, $\Sigma$.

  \noindent Alternatively, this transition system can be viewed as a kind of proof system in an unlabeled lattice. This is equivalent to Schultz and Mihov's automaton, but more amenable to our purposes as it does not any contain $\varepsilon$-arcs, and uses skip connections to represent consecutive deletions.

  \begin{prooftree}
    \AxiomC{$s\in\Sigma \phantom{\land} i \in [0, n] \phantom{\land} j \in [1, k]$}
    \RightLabel{$\duparrow$}
    \UnaryInfC{$(q_{i, j-1} \overset{s}{\rightarrow} q_{i,j}) \in \delta$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$s\in\Sigma \phantom{\land} i \in [1, n] \phantom{\land} j \in [1, k]$}
    \RightLabel{$\ddiagarrow$}
    \UnaryInfC{$(q_{i-1, j-1} \overset{s}{\rightarrow} q_{i,j}) \in \delta$}
  \end{prooftree}
  \begin{prooftree}
    \AxiomC{$i \in [1, n] \phantom{\land} j \in [0, k]$}
    \RightLabel{$\drightarrow$}
    \UnaryInfC{$(q_{i-1, j} \overset{\sigma_i}{\rightarrow} q_{i,j}) \in \delta$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$d \in [1, d_{\max}] \phantom{\land} i \in [d + 1, n] \phantom{\land} j \in [d, k]$}
    \RightLabel{$\knightarrow$}
    \UnaryInfC{$(q_{i-d-1, j-d} \overset{\sigma_i}{\rightarrow} q_{i,j}) \in \delta$}
  \end{prooftree}
  \begin{prooftree}
    \AxiomC{$\vphantom{|}$}
    \RightLabel{$\textsc{Init}$}
    \UnaryInfC{$q_{0,0} \in I$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$q_{i, j}$}
    \AxiomC{$|n-i+j| \leq k$}
    \RightLabel{$\textsc{Done}$}
    \BinaryInfC{$q_{i, j}\in F$}
  \end{prooftree}

  \newcommand{\substitutionExample}{
    \tikz{
      \foreach \x in {0,8,16,24,32,40}{
        \fill (\x pt,0pt) circle [radius = 1pt];
        \fill (\x pt,8pt) circle [radius = 1pt];
      }
      \phantom{\fill (0pt,-8pt) circle [radius = 1pt];}
      \draw [-to] (0pt,0pt) -- (8pt,0pt);
      \draw [-to] (8pt,0pt) -- (16pt,0pt);
      \draw [-to] (16pt,0pt) -- (24pt,8pt);
      \draw [-to] (24pt,8pt) -- (32pt,8pt);
      \draw [-to] (32pt,8pt) -- (40pt,8pt);
    }
  }

  \newcommand{\insertionExample}{
    \tikz{
      \foreach \x in {0,8,16,24,32,40}{
        \fill (\x pt,0pt) circle [radius = 1pt];
        \fill (\x pt,8pt) circle [radius = 1pt];
      }
      \phantom{\fill (0pt,-8pt) circle [radius = 1pt];}
      \fill[white] (16pt,0pt) circle [radius = 1.2pt];
      \fill[white] (24pt,8pt) circle [radius = 1.2pt];
      \draw [-to] (0pt,0pt) -- (8pt,0pt);
      \draw [-to] (8pt,0pt) -- (24pt,0pt);
      \draw [-to] (24pt,0pt) -- (16pt,8pt);
      \draw [-to] (16pt,8pt) -- (32pt,8pt);
      \draw [-to] (32pt,8pt) -- (40pt,8pt);
    }
  }

  \newcommand{\deletionExample}{
    \tikz{
      \foreach \x in {0,8,16,24,32,40}{
        \fill (\x pt,0pt) circle [radius = 1pt];
        \fill (\x pt,8pt) circle [radius = 1pt];
      }
      \phantom{\fill (0pt,-8pt) circle [radius = 1pt];}
      \draw [-to] (0pt,0pt) -- (8pt,0pt);
      \draw [-to] (8pt,0pt) -- (16pt,0pt);
      \draw [-to] (16pt,0pt) -- (24pt,0pt);
      \draw [-to] (24pt,0pt) -- (40pt,8pt);
    }
  }

  \newcommand{\doubleDeletionExample}{
    \tikz{
      \foreach \x in {0,8,16,24,32,40}{
        \fill (\x pt,0pt) circle [radius = 1pt];
        \fill (\x pt,8pt) circle [radius = 1pt];
        \fill (\x pt,16pt) circle [radius = 1pt];
      }
      \draw [-to] (0pt,0pt) -- (24pt,16pt);
      \draw [-to] (24pt,16pt) -- (32pt,16pt);
      \draw [-to] (32pt,16pt) -- (40pt,16pt);
    }
  }

  \newcommand{\subDelExample}{
    \tikz{
      \foreach \x in {0,8,16,24,32,40}{
        \fill (\x pt,0pt) circle [radius = 1pt];
        \fill (\x pt,8pt) circle [radius = 1pt];
        \fill (\x pt,16pt) circle [radius = 1pt];
      }
      \draw [-to] (0pt,0pt) -- (8pt,0pt);
      \draw [-to] (8pt,0pt) -- (16pt,8pt);
      \draw [-to] (16pt,8pt) -- (32pt,16pt);
      \draw [-to] (32pt,16pt) -- (40pt,16pt);
    }
  }

  \newcommand{\subSubExample}{
    \tikz{
      \foreach \x in {0,8,16,24,32,40}{
        \fill (\x pt,0pt) circle [radius = 1pt];
        \fill (\x pt,8pt) circle [radius = 1pt];
        \fill (\x pt,16pt) circle [radius = 1pt];
      }
      \draw [-to] (0pt,0pt) -- (8pt,0pt);
      \draw [-to] (8pt,0pt) -- (16pt,8pt);
      \draw [-to] (16pt,8pt) -- (24pt,16pt);
      \draw [-to] (24pt,16pt) -- (32pt,16pt);
      \draw [-to] (32pt,16pt) -- (40pt,16pt);
    }
  }

  \newcommand{\insertDeleteExample}{
    \tikz{
      \foreach \x in {0,8,16,24,32,40,48}{
        \fill (\x pt,0pt) circle [radius = 1pt];
        \fill (\x pt,8pt) circle [radius = 1pt];
        \fill (\x pt,16pt) circle [radius = 1pt];
      }
      \fill[white] (16pt,16pt) circle [radius = 1.2pt];
      \fill[white] (8pt,0pt) circle [radius = 1.2pt];
      \fill[white] (16pt,8pt) circle [radius = 1.2pt];
      \draw [-to] (0pt,0pt) -- (16pt,0pt);
      \draw [-to] (16pt,0pt) -- (8pt,8pt);
      \draw [-to] (8pt,8pt) -- (24pt,8pt);
      \draw [-to] (24pt,8pt) -- (40pt,16pt);
      \draw [-to] (40pt,16pt) -- (48pt,16pt);
    }
  }

  Each arc plays a specific role. $\duparrow$ handles insertions, $\ddiagarrow$ handles substitutions, $\duparrow$ handles insertions and $\knightarrow$ handles [consecutive] deletions of various lengths. Let us consider some illustrative cases.

  \begin{table}[h!]
    \begin{tabular}{ccccccc}

      \texttt{f\hspace{3pt}.\hspace{3pt}\hlorange{[}\hspace{3pt}x\hspace{3pt})} &
      \texttt{f\hspace{3pt}.\hspace{3pt}\phantom{(}\hspace{3pt}x\hspace{3pt})} &
      \texttt{f\hspace{3pt}.\hspace{3pt}(\hspace{3pt}\hlred{x}\hspace{3pt})} &
      \texttt{\hlred{.}\hspace{3pt}\hlred{+}\hspace{3pt}(\hspace{3pt}x\hspace{3pt})} &
      \texttt{f\hspace{3pt}\hlorange{.}\hspace{3pt}\hlred{(}\hspace{3pt}x\hspace{3pt};} &
      \texttt{[\hspace{3pt}\hlorange{,}\hspace{3pt}\hlorange{x}\hspace{3pt}y\hspace{3pt}]} &
      \texttt{[\hspace{3pt}\phantom{,}\hspace{3pt},\hspace{3pt}\hlred{x}\hspace{3pt}y\hspace{3pt}]} \\

      \texttt{f\hspace{3pt}.\hspace{3pt}\hlorange{(}\hspace{3pt}x\hspace{3pt})} &
      \texttt{f\hspace{3pt}.\hspace{3pt}\hlgreen{(}\hspace{3pt}x\hspace{3pt})} &
      \texttt{f\hspace{3pt}.\hspace{3pt}(\hspace{3pt}\phantom{x}\hspace{3pt})} &
      \texttt{\phantom{f}\hspace{3pt}\phantom{.}\hspace{3pt}(\hspace{3pt}x\hspace{3pt})} &
      \texttt{f\hspace{3pt}\hlorange{*}\hspace{3pt}\phantom{(}\hspace{3pt}x\hspace{3pt};} &
      \texttt{[\hspace{3pt}\hlorange{x}\hspace{3pt}\hlorange{,}\hspace{3pt}y\hspace{3pt}]} &
      \texttt{[\hspace{3pt}\hlgreen{x}\hspace{3pt},\hspace{3pt}\phantom{x}\hspace{3pt}y\hspace{3pt}]} \\

      \substitutionExample & \insertionExample & \deletionExample & \doubleDeletionExample & \subDelExample & \subSubExample & \insertDeleteExample
    \end{tabular}
  \end{table}

  Note that the same patch can have multiple Levenshtein alignments. $\textsc{Done}$ constructs the final states, which are all states accepting strings $\sigma'$ such that Levenshtein distance of $\Delta(\sigma, \sigma') \leq d_\max$.

  To avoid creating a parallel bundle of arcs for each insertion and substutition point, we instead decorate each arc with a nominal predicate, accepting or rejecting $\sigma_i$. To distinguish this nominal variant from the original construction, we highlight the modified rules in orange below.

  \begin{prooftree}
    \AxiomC{$i \in [0, n] \phantom{\land} j \in [1, k]$}
    \RightLabel{$\duparrow$}
    \UnaryInfC{$(q_{i, j-1} \overset{{\color{orange}[\neq \sigma_i]}}{\rightarrow} q_{i,j}) \in \delta$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$i \in [1, n] \phantom{\land} j \in [1, k]$}
    \RightLabel{$\ddiagarrow$}
    \UnaryInfC{$(q_{i-1, j-1} \overset{{\color{orange}[\neq \sigma_i]}}{\rightarrow} q_{i,j}) \in \delta$}
  \end{prooftree}
  \begin{prooftree}
    \AxiomC{$i \in [1, n] \phantom{\land} j \in [0, k]$}
    \RightLabel{$\drightarrow$}
    \UnaryInfC{$(q_{i-1, j} \overset{{\color{orange}[=\sigma_i]}}{\rightarrow} q_{i,j}) \in \delta$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$d \in [1, d_{\max}] \phantom{\land} i \in [d + 1, n] \phantom{\land} j \in [d, k]$}
    \RightLabel{$\knightarrow$}
    \UnaryInfC{$(q_{i-d-1, j-d} \overset{{\color{orange}[=\sigma_i]}}{\rightarrow} q_{i,j}) \in \delta$}
  \end{prooftree}

  Nominalizing the NFA eliminates the creation of $e=|\Sigma - 1|\cdot|\sigma|\cdot2d_\max$ unnecessary arcs over the entire Levenshtein automaton and drastically reduces the size of the construction to follow, but does not affect the underlying semantics. Thus, it is essential to first nominalize the automaton before proceeding to avoid a large blowup in the intermediate grammar.

  \subsection{Levenshtein-Bar-Hillel Construction}\label{sec:lev_bh}

  We now describe the Bar-Hillel construction, which generates a grammar recognizing the intersection between a regular and a context-free language, then specialize it to Levenshtein intersections.

  \begin{lemma}\label{lemma:bar-hillel}
  For any context-free language $\ell$ and finite state automaton $\alpha$, there exists a context-free grammar $G_\cap$ such that $\mathcal{L}(G_\cap) = \ell \cap \mathcal{L}(\alpha)$. See Bar-Hillel~\cite{bar1961formal}.
  \end{lemma}

  \noindent Although Bar-Hillel~\cite{bar1961formal} lacks an explicit construction, Beigel and Gasarch~\cite{beigelproof} construct $G_\cap$ like so:

  \begin{prooftree}
    \AxiomC{$q \in I \phantom{\land} r \in F\vphantom{\overset{a}{\rightarrow}}$}
%  \RightLabel{$\varepsilon^+\textsc{-int}$}
    \UnaryInfC{$\big(S\rightarrow q S r\big) \in P_\cap$}
    \DisplayProof
    \hskip 1em
    \AxiomC{$(A \rightarrow a) \in P$}
    \AxiomC{$(q\overset{a}{\rightarrow}r) \in \delta$}
%  \RightLabel{$\varepsilon^+\textsc{-int}$}
    \BinaryInfC{$\big(qAr\rightarrow a\big)\in P_\cap$}
    \DisplayProof
    \hskip 1em
%\end{prooftree}
%\begin{prooftree}
    \AxiomC{$(w \rightarrow xz) \in P\vphantom{\overset{a}{\rightarrow}}$}
    \AxiomC{$p,q,r \in Q$}
    \RightLabel{\Join}
    \BinaryInfC{$\big(pwr\rightarrow (pxq)(qzr)\big) \in P_\cap$}
  \end{prooftree}

  This, now standard, Bar-Hillel construction applies to any CFL and REG language intersection, but generates a grammar whose cardinality is approximately $|P_\cap|=|I|\cdot|F| + |P|\cdot|\Sigma|\cdot|\sigma|\cdot2d_{\max} + |P|\cdot|Q|^3$. Applying the BH construction directly to practical languages and code snippets can generate hundreds of trillions of productions for even moderately sized grammars and Levenshtein automata. Instead, we will describe a kind of reachability analysis that elides many unreachable productions in the case of Levenshtein intersection.

  Consider $\Join$, the most expensive rule. What $\Join$ tells us is each nonterminal in the intersection grammar, $P_\cap$, matches a substring simultaneously recognized by (1) a pair of states in the original NFA and (2) a nonterminal in the original CFG. A key observation is that $\Join$ considers the intersection between every such triple, but this is a gross overapproximation for most NFAs and CFGs, as the vast majority of all state pairs and nonterminals recognize no strings in common.

  To identify these triples, we define an interval domain that soundly overapproximates the Parikh image, encoding the minimum and maximum number of terminals each nonterminal can generate. Since some intervals may be right-unbounded, we write $\mathbb{N}^*=\mathbb{N} \cup \{\infty\}$ to denote the upper bound, and $\Pi = \{[a, b] \in \mathbb{N} \times \mathbb{N}^* \mid a \leq b\}^{|\Sigma|}$ to denote the Parikh map of all terminals.

  \begin{definition}[Parikh mapping of a nonterminal]
    Let $p: \Sigma^*\rightarrow\mathbb{N}^{|\Sigma|}$ be the Parikh operator~\cite{parikh1966context}, which counts the frequency of terminals in a string. Let $\pi: V \rightarrow \Pi$ be a function returning the smallest interval such that $\forall \sigma: \Sigma^*, \forall v: V$, $v \Rightarrow^* \sigma \vdash p(s) \in \pi(v)$.
  \end{definition}

  In other words, the Parikh mapping computes the greatest lower and least upper bound of the Parikh image over all strings in the language of a nonterminal. The infimum of a nonterminal's Parikh interval tells us how many of each terminal a nonterminal \textit{must} generate, and the supremum tells us how many it \textit{can} generate. Likewise, we define a similar relation over NFA state pairs:

  \begin{definition}[Parikh mapping of NFA states]
    We define $\pi: Q\times Q \rightarrow \Pi$ as returning the smallest interval such that $\forall \sigma: \Sigma^*, \forall q, q': Q$, $q \overset{\sigma}{\Longrightarrow} q' \vdash p(\sigma) \in \pi(q, q')$.
  \end{definition}

  Next, we will define a measure on Parikh intervals, which will represent the minimum total edits required to transform a string in one Parikh interval to a string in another.

  \begin{definition}[Parikh divergence]
    Given two Parikh intervals $\pi, \pi': \Pi$, we define the divergence between them as $\pi \parallel \pi' = \sum_{n=1}^{|\Sigma|} \min_{(i, i') \in \pi[n]\times \pi'[n]} |i - i'|$.
  \end{definition}

  Now, we know that if the Parikh divergence between two intervals exceeds the Levenshtein margin between two states in a Lev-NFA, those intervals must be incompatible as no two strings, one from each Parikh interval, can be transformed into the other with fewer than $\pi \parallel \pi'$ edits.

  \begin{definition}[Levenshtein-Parikh compatibility]
    Let $q = q_{h,i}, q'=q_{j,k}$ be two states in a Lev-NFA and V be a CFG nonterminal. We say that $(q, v, q'): Q\times V\times Q$ are compatible iff the Parikh divergence is bounded by the Levenshtein margin $k-i$, i.e., $v \lhd qq' \iff (\pi(v) \parallel \pi(q, q')) \leq k-i$.
  \end{definition}

  We define the modified Bar-Hillel construction for Levenshtein intersections as follows:

\begin{prooftree}
  \def\defaultHypSeparation{\hskip 0.15cm}
  \AxiomC{$(A \rightarrow a) \in P$}
  \AxiomC{$\color{orange}S.a$}
  \AxiomC{$(q\overset{{\color{orange}S}}{\rightarrow}r) \in \delta$}
%  \RightLabel{$\varepsilon^+\textsc{-int}$}
  \TrinaryInfC{$\big(qAr\rightarrow a\big)\in P_\cap$}
  \DisplayProof
  \AxiomC{$\vphantom{\overset{S}{\rightarrow}}\color{orange} w \lhd pr \phantom{\land} x \lhd pq \phantom{\land} z \lhd qr$}
  \AxiomC{$(w \rightarrow xz) \in P\vphantom{\overset{a}{\rightarrow}}$}
  \AxiomC{$p,q,r \in Q$}
  \RightLabel{\Join}
  \TrinaryInfC{$\big(pwr\rightarrow (pxq)(qzr)\big) \in P_\cap$}
\end{prooftree}

  Finally, once $G_\cap$ is constructed, it is renormalized by removing all unreachable and non-generating productions following~\cite{firsov2015certified} to obtain $G_\cap^*$, which is usually several orders of magnitude smaller. When this grammar is sufficiently small, we can extract all relevant repairs from it, otherwise we sample from it to obtain a subset of candidate repairs.

%Specifically, we compute Parikh intervals generated by every path though the Levenshtein automaton, then intersect the Parikh intervals for the candidate nonterminals in question. For example, suppose we have a $p, q, r: Q$ and $w \rightarrow x z$. Let's check...

%To generate edits from it, we can use the same procedure as before, but instead of interleaving $\err\sigma$ with $\varepsilon$ and introducing holes, we simply use $A\big((\_)^{|\err{\sigma}| + d}\big, G_\cap)$.


  \subsection{Repair as idempotent matrix completion}\label{sec:matrix_completion}

  Now that we have a language that recognizes local repairs, we need a method to extract the repairs themselves. We impose specific criteria on such a procedure: it must generate (1) only valid repairs and (2) all repairs in the language. We also require the sampler to have (3) parallel decomposability and generate samples (4) uniformly, both (5) with and (6) without replacement in (7) constant time and space. These requirements motivate the need for a specialized generating function.

  In this section, we will introduce the porous completion problem and show how it can be translated to a kind of idempotent matrix completion, whose roots are valid strings in a context-free language. This technique is convenient for its geometric interpretability, amenability to parallelization, and generalizability to any CFG, regardless of finitude or ambiguity. We will see how, by redefining the algebraic operations $\oplus, \otimes$ over different carrier sets, one can obtain a recognizer, parser, generator, Parikh map and other convenient structures for working with CFLs.

  Given a CFG, $G' : \mathcal{G}$ in Chomsky Normal Form (CNF), we can construct a recognizer $R: \mathcal{G} \rightarrow \Sigma^n \rightarrow \mathbb{B}$ for strings $\sigma: \Sigma^n$ as follows. Let $2^V$ be our domain, $0$ be $\varnothing$, $\oplus$ be $\cup$, and $\otimes$ be defined as:\vspace{-10pt}

  \begin{align}
    X \otimes Z \coloneqq \big\{\;w \mid \langle x, z\rangle \in X \times Z, (w\rightarrow xz) \in P\;\big\}
  \end{align}

  \noindent If we define $\hat\sigma_r \coloneqq \{w \mid (w \rightarrow \sigma_r) \in P\}$, then construct a matrix with nonterminals on the superdiagonal representing each token, $M_0[r+1=c](G', \sigma) \coloneqq \;\hat\sigma_r$, the fixpoint $M_{i+1} = M_i + M_i^2$ is uniquely determined by the superdiagonal entries, which are computed as follows:\vspace{-10pt}

  \begin{align*}
    M_0\coloneqq
    \begin{pNiceMatrix}[nullify-dots,xdots/line-style=loosely dotted]
      \varnothing & \hat\sigma_1 & \varnothing & \Cdots & \varnothing \\
      \Vdots      & \Ddots       & \Ddots      & \Ddots & \Vdots\\
                  &              &             &        & \varnothing\\
                  &              &             &        & \hat\sigma_n \\
      \varnothing & \Cdots       &             &        & \varnothing
    \end{pNiceMatrix} &\Rightarrow
    \begin{pNiceMatrix}[nullify-dots,xdots/line-style=loosely dotted]
      \varnothing & \hat\sigma_1 & \Lambda & \Cdots & \varnothing \\
      \Vdots      & \Ddots       & \Ddots  & \Ddots & \Vdots\\
                  &              &         &        & \Lambda\\
                  &              &         &        & \hat\sigma_n \\
      \varnothing & \Cdots       &         &        & \varnothing
    \end{pNiceMatrix} &\Rightarrow \ldots \Rightarrow M_\infty =
    \begin{pNiceMatrix}[nullify-dots,xdots/line-style=loosely dotted]
      \varnothing & \hat\sigma_1 & \Lambda & \Cdots & \Lambda^*_\sigma\\
      \Vdots      & \Ddots       & \Ddots  & \Ddots & \Vdots\\
                  &              &         &        & \Lambda\\
                  &              &         &        & \hat\sigma_n \\
      \varnothing & \Cdots       &         &        & \varnothing
    \end{pNiceMatrix}
  \end{align*}

  \noindent The northeasternmost entry $\Lambda^*_\sigma$ gives us the recognizer $R(G', \sigma) \coloneqq [S \in \Lambda^*_\sigma] \Leftrightarrow [\sigma \in \mathcal{L}(G)]$~\footnote{Hereinafter, we use Iverson brackets to denote the indicator function of a predicate with free variables, i.e., $[P] \Leftrightarrow \mathds{1}(P)$.}.

  This procedure can be lifted to the domain of strings containing free variables, which we call the \textit{porous completion problem}, whose solutions are the set of all syntactically valid strings consistent with the input string.

  \begin{definition}[Porous completion]
    Let $\underline\Sigma \coloneqq \Sigma \cup \{\_\}$, where $\_$ denotes a hole. We denote $\sqsubseteq: \Sigma^n \times \underline\Sigma^n$ as the relation $\{\langle\sigma', \sigma\rangle \mid \sigma_i \in \Sigma \implies \sigma_i' = \sigma_i\}$ and the set of all inhabitants $\{\sigma': \Sigma^+ \mid \sigma' \sqsubseteq \sigma\}$ as $\text{H}(\sigma)$. Given a \textit{porous string}, $\sigma: \underline\Sigma^*$ we seek all syntactically valid inhabitants, i.e., $A(\sigma)\coloneqq\text{H}(\sigma)\cap\ell$.
  \end{definition}

  Let us consider an example with two holes, $\sigma = 1$ \_ \_, and the grammar being $G\coloneqq\{S\rightarrow N O N, O \rightarrow + \mid \times, N \rightarrow 0 \mid 1\}$. This can be rewritten into CNF as $G'\coloneqq \{S \rightarrow N L, N \rightarrow 0 \mid 1, O \rightarrow \times \mid +, L \rightarrow O N\}$. Using the algebra where $\oplus=\cup$, $X \otimes Z = \big\{\;w \mid \langle x, z\rangle \in X \times Z, (w\rightarrow xz) \in P\;\big\}$, the fixpoint $M' = M + M^2$ can be computed as follows, shown in the leftmost column:\\

  \begin{small}
  {\renewcommand{\arraystretch}{1.2}
  \noindent\phantom{...}\begin{tabular}{|c|c|c|c|}
                          \hline
                          & $2^V$ & $\mathbb{Z}_2^{|V|}$ & $\mathbb{Z}_2^{|V|}\rightarrow\mathbb{Z}_2^{|V|}$\\\hline
                          $M_0$ & \begin{pmatrix}
                                    \phantom{V} & \tiny{\{N\}} &         &             \\
                                    &              & \{N,O\} &             \\
                                    &              &         & \{N,O\} \\
                                    &              &         &
                          \end{pmatrix} & \begin{pmatrix}
                                            \phantom{V} & \ws\bs\ws\ws &              &              \\
                                            &              & \ws\bs\bs\ws &              \\
                                            &              &              & \ws\bs\bs\ws \\
                                            &              &              &
                          \end{pmatrix} & \begin{pmatrix}
                                            \phantom{V} & V_{0, 1} &          &          \\
                                            &          & V_{1, 2} &          \\
                                            &          &          & V_{2, 3} \\
                                            &          &          &
                          \end{pmatrix} \\\hline
                          $M_1$ & \begin{pmatrix}
                                    \phantom{V} & \tiny{\{N\}} & \varnothing &         \\
                                    &              & \{N,O\}     & \{L\}   \\
                                    &              &             & \{N,O\} \\
                                    &              &             &
                          \end{pmatrix} & \begin{pmatrix}
                                            \phantom{V} & \ws\bs\ws\ws & \ws\ws\ws\ws &              \\
                                            &              & \ws\bs\bs\ws & \bs\ws\ws\ws \\
                                            &              &              & \ws\bs\bs\ws \\
                                            &              &              &
                          \end{pmatrix} & \begin{pmatrix}
                                            \phantom{V} & V_{0, 1} & V_{0, 2} &          \\
                                            &          & V_{1, 2} & V_{1, 3} \\
                                            &          &          & V_{2, 3} \\
                                            &          &          &
                          \end{pmatrix} \\\hline
                          $M_\infty$ & \begin{pmatrix}
                                         \phantom{V} & \tiny{\{N\}} & \varnothing & \{S\}   \\
                                         &              & \{N,O\}     & \{L\}   \\
                                         &              &             & \{N,O\} \\
                                         &              &             &
                          \end{pmatrix} & \begin{pmatrix}
                                            \phantom{V} & \ws\bs\ws\ws & \ws\ws\ws\ws & \ws\ws\ws\bs \\
                                            &              & \ws\bs\bs\ws & \bs\ws\ws\ws \\
                                            &              &              & \ws\bs\bs\ws \\
                                            &              &              &
                          \end{pmatrix} & \begin{pmatrix}
                                            \phantom{V} & V_{0, 1} & V_{0, 2} & V_{0, 3} \\
                                            &          & V_{1, 2} & V_{1, 3} \\
                                            &          &          & V_{2, 3} \\
                                            &          &          &
                          \end{pmatrix}\\\hline
  \end{tabular}\\
  }
  \end{small}

  The same procedure can be translated, without loss of generality, into the bit domain ($\mathbb{Z}_2^{|V|}$) using a lexicographic ordering, however $M_\infty$ in both $2^V$ and $\mathbb{Z}_2^{|V|}$ represents a decision procedure, i.e., $[S\in V_{0, 3}]\Leftrightarrow [V_{0, 3, 3}=\bs] \Leftrightarrow [A(\sigma) \neq \varnothing]$. Since $V_{0, 3} = \{S\}$, we know there exists at least one $\sigma' \in A$, but $M_\infty$ does not reveal its identity.

%$\{\text{xor}, \land, \top\}$ is a functionally complete set is equivalent to $\mathbb{Z}_2$ $\top := 1, \land := \times, \text{xor} := +$. We can define $=$ as $(a = b) \Leftrightarrow (a \text{ xor } b) \text{ xor } \top \Leftrightarrow (a + b) + \top$.

  In order to extract the inhabitants, we can translate the bitwise procedure into an equation with free variables. Here, we can encode the idempotency constraint directly as $M = M^2$. We first define $X \boxtimes Z = [X_2 \land Z_1, \bot, \bot, X_1 \land Z_0]$ and $X \boxplus Z = [X_i \lor Z_i]_{i \in [0, |V|)}$. Since the unit nonterminals $O, N$ can only occur on the superdiagonal, they may be safely ignored by $\boxtimes$. To solve for $M_\infty$, we proceed by first computing $V_{0, 2}, V_{1, 3}$ as follows:

  \begin{small}
  \begin{align*}
    V_{0, 2} &= V_{0, j} \cdot V_{j, 2} = V_{0, 1} \boxtimes V_{1, 2}                         &  V_{1, 3} &= V_{1, j} \cdot V_{j, 3} = V_{1, 2} \boxtimes V_{2, 3}\\
    &= [L \in V_{0, 2}, \bot, \bot, S \in V_{0, 2}]                                           &  &= [L \in V_{1, 3}, \bot, \bot, S \in V_{1, 3}]\\
    &= [O \in V_{0, 1} \land N \in V_{1, 2}, \bot, \bot, N \in V_{0, 1} \land L \in V_{1, 2}] &  &= [O \in V_{1, 2} \land N \in V_{2, 3}, \bot, \bot, N \in V_{1, 2} \land L \in V_{2, 3}]\\
    &= [V_{0, 1, 2} \land V_{1, 2, 1}, \bot, \bot, V_{0, 1, 1} \land V_{1, 2, 0}]             &  &= [V_{1, 2, 2} \land V_{2, 3, 1}, \bot, \bot, V_{1, 2, 1} \land V_{2, 3, 0}]
  \end{align*}
  \end{small}

  Now we solve for the corner entry $V_{0, 3}$ by taking the bitwise dot product between the first row and last column, yielding:

  \begin{align*}
    V_{0, 3} &= V_{0, j} \cdot V_{j, 3} = V_{0, 1} \boxtimes V_{1, 3} \boxplus V_{0, 2} \boxtimes V_{2, 3}\\
%  &= [V_{0, 1, 2} \land V_{1, 3, 1}, \bot, \bot, V_{0, 1, 1} \land V_{1, 3, 0}] + [V_{0, 2, 2} \land V_{2, 3, 1}, \bot, \bot, V_{0, 2, 1} \land V_{2, 3, 0}]\\
    &= [V_{0, 1, 2} \land V_{1, 3, 1} \lor V_{0, 2, 2} \land V_{2, 3, 1}, \bot, \bot, V_{0, 1, 1} \land V_{1, 3, 0} \lor V_{0, 2, 1} \land V_{2, 3, 0}]
  \end{align*}

  \noindent Since we only care about $V_{0, 3, 3} \Leftrightarrow [S \in V_{0, 3}]$, so we can ignore the first three entries and solve for:

  \begin{align*}
    V_{0, 3, 3} &= V_{0, 1, 1} \land V_{1, 3, 0} \lor V_{0, 2, 1} \land V_{2, 3, 0}\\
    &= V_{0, 1, 1} \land (V_{1, 2, 2} \land V_{2, 3, 1}) \lor V_{0, 2, 1} \land \bot\\
    &= V_{0, 1, 1} \land V_{1, 2, 2} \land V_{2, 3, 1}\\
    &= [N \in V_{0, 1}] \land [O \in V_{1, 2}] \land [N \in V_{2, 3}]
  \end{align*}

  Now we know that $\sigma =$ 1 \underline{O} \underline{N} is a valid solution, and we can take the product $\{1\}\times \hat\sigma_2^{-1}(O) \times \hat\sigma_3^{-1}(N)$ to recover the admissible set, yielding $A=\{1+0, 1+1, 1\times 0, 1\times 1\}$. In this case, since $G$ is unambiguous, there is only one parse tree satisfying $V_{0, |\sigma|, 3}$, but in general, there can be multiple valid parse trees.

  \subsection{An algebraic datatype for context-free parse forests}\label{sec:ptree}

  This procedure generates solutions satisfying the matrix fixpoint, but forgets their provenance. The question naturally arises, is there a way to solve for the parse trees directly? This would allow us to handle ambiguous grammars, whilst preserving the natural structure of the parsing domain.

  \begin{wrapfigure}{r}{0.47\textwidth}
    \vspace{-5pt}
    \resizebox{0.47\textwidth}{!}{
      \begin{tikzpicture}
      [
        grow                    = right,
        sibling distance        = 3em,
        level distance          = 5em,
        edge from parent/.style = {draw, -latex},
        every node/.style       = {font=\footnotesize},
        sloped,
        treenode/.style = {shape=rectangle, rounded corners,
        draw, align=center,
        top color=white, bottom color=blue!20},
        root/.style     = {treenode, font=\tiny, bottom color=red!30},
        env/.style      = {treenode, font=\tiny},
        dummy/.style    = {circle,draw}
      ]
        \node [root] {S}
        child { node [env] {BC}
        child { node [root] {B}
        child { node [env] {RD}
        child { node [root] {R} edge from parent node [above] }
        child { node [root] {D} edge from parent node [above] }
        edge from parent node [above] }
        edge from parent node [below] }
        child { node [root] {C}
        child { node [env] {\ldots\vphantom{BB}} edge from parent node [below] }
%  child { edge from parent node [above] {\ldots} }
        edge from parent node [below] }
        edge from parent node [above] }
        child { node [env] {\ldots\vphantom{BB}} edge from parent node [below] }
        child { node [env] {AB}
        child { node [root] {A}
        child {
          node [env] {QC}
          child { node [root] {Q} edge from parent node [above] }
          child { node [root] {C} edge from parent node [above] }
          edge from parent node [above]
        }
%    child { node [env] {ZQ} edge from parent node [above] }
        child { node [env] {\ldots\vphantom{BB}} edge from parent node [below] }
        edge from parent node [below] }
        child { node [root] {B}
        child { node [env] {RD}
        child { node [root] {R} edge from parent node [above] }
        child { node [root] {D} edge from parent node [above] }
        edge from parent node [above] }
        edge from parent node [below] }
        edge from parent node [above] };
      \end{tikzpicture}
    }
    \caption{A partial $\mathbb{T}_2$ corresponding to the grammar $\{S \rightarrow BC \mid \ldots \mid AB, B\rightarrow RD \mid \ldots, A\rightarrow QC \mid \ldots\}$.}
    \label{fig:ptree}
    \vspace{-10pt}
  \end{wrapfigure}

  We will now describe a datatype for compactly representing CFL parse forests, then redefine the semiring algebra over this domain. This construction is especially convenient for tracking provenance under ambiguity, Parikh mapping, counting the size of a finite CFL, and sampling trees either with replacement using a PCFG, or without replacement using an integer pairing function.

  We first define a datatype $\mathbb{T}_3 = (V \cup \Sigma) \rightharpoonup \mathbb{T}_2$ where $\mathbb{T}_2 = (V \cup \Sigma) \times (\mathbb{N} \rightharpoonup \mathbb{T}_2\times\mathbb{T}_2)$\footnote{Given a $T:\mathbb{T}_2$, we may also refer to $\pi_1(T), \pi_2(T)$ as $\texttt{root}(T)$ and $\texttt{children}(T)$ respectively.}. Morally, we can think of $\mathbb{T}_2$ as an implicit set of possible trees that can be generated by a CFG in CNF, consistent with a finite-length porous string. Structurally, we may interpret $\mathbb{T}_2$ as an algebraic data type corresponding to the fixpoints of the following recurrence. This tells us each $\mathbb{T}_2$ can be a terminal, nonterminal, or a nonterminal and a sequence of nonterminal pairs and their two children:\vspace{-10pt}

  \begin{equation}
    L(p) = 1 + p L(p) \phantom{addspace} P(a) = \Sigma + V + V L\big(V^2P(a)^2\big)
  \end{equation}

  Given a $\sigma: \underline\Sigma$, we construct $\mathbb{T}_2$ from the bottom-up, and sample from the top-down. Depicted in Fig.~\ref{fig:ptree} is a partial $\mathbb{T}_2$, where red nodes are \texttt{root}s and blue nodes are \texttt{children}.

  We construct the first upper diagonal $\hat\sigma_r = \Lambda(\sigma_r)$ as follows:

\vspace{-10pt}\begin{equation}
  \begin{footnotesize}
\Lambda(s: \underline\Sigma) \mapsto \begin{cases}
\bigoplus_{s'\in \Sigma} \Lambda(s') & \text{if $s$ is a hole,} \vspace{5pt}\\
\big\{\mathbb{T}_2\big(w, \big[\langle\mathbb{T}_2(s), \mathbb{T}_2(\varepsilon)\rangle\big]\big) \mid (w \rightarrow s)\in P\big\} & \text{otherwise.}
\end{cases}
  \end{footnotesize}
\end{equation}

\noindent This initializes the superdiagonal entries, enabling us to compute the fixpoint $M_\infty$ in the same manner described in \S~\ref{sec:matrix_completion} by redefining $\oplus, \otimes: \mathbb{T}_3 \times \mathbb{T}_3 \rightarrow \mathbb{T}_3$ as:

\begin{align}
  X \oplus Z &\mapsto \bigcup_{\mathclap{k\in \pi_1(X \cup Z)}}\Big\{k \Rightarrow \mathbb{T}_2(k, x \cup z) \mid x \in \pi_2(X\circ k), z \in \pi_2(Z\circ k)\Big\}\\
  X \otimes Z &\mapsto \bigoplus_{\mathclap{(w\rightarrow xz) \in P}}\Big\{\mathbb{T}_2\Big(w, \big[\langle X\circ x, Z\circ z\rangle\big]\Big) \mid x \in \pi_1(X), z \in \pi_1(Z)\Big\}
\end{align}

These operators group subtrees by their root nonterminal, then aggregate their children. Instead of tracking sets, each $\Lambda$ now becomes a dictionary of $\mathbb{T}_2$, indexed by their root nonterminals.

  $\mathbb{T}_2$ is a convenient datatype for many operations involving CFGs. We can use it to approximate the Parikh image, compute the size of a finite CFG, and sample parse trees with or without replacement. For example, to obtain the Parikh map, we may use the following recurrence,

\begin{equation}
  \pi(T: \mathbb{T}_2) \mapsto \begin{cases}
%    \big|\{s \mid \big(\texttt{root}(T) \rightarrow s\big) \in P^\cap\}\big| & \text{if $T$ is a leaf,} \\
  \big[[1, 1] \text{ if } \texttt{root}(T) = s \text{ else } [0, 0]\big]_{s\in \Sigma}  & \text{if $T$ is a leaf,} \\
  \bigoplus_{\langle T_1, T_2\rangle \in \texttt{children}(T)} \pi(T_1) \otimes \pi(T_2) & \text{otherwise.}
  \end{cases}
\end{equation}

  %infix fun IntRange.merge(other: IntRange) =
  %  minOf(start, other.first)..maxOf(last, other.last)
  %
  %operator fun ParikhBounds.plus(other: ParikhBounds) =
  %  (keys + other.keys).associateWith {
  %    (get(it) ?: 0..0) merge (other[it] ?: 0..0)
  %  }
  %
  %operator fun ParikhBounds.times(other: ParikhBounds) =
  %  (keys + other.keys).associateWith {
  %    (get(it) ?: 0..0) join (other[it] ?: 0..0)
  %  }
  %
  %infix fun IntRange.join(other: IntRange) =
  %  (first + other.first)..(last + other.last)

  \noindent where the operations over Parikh maps $\oplus, \otimes: \Pi \times \Pi \rightarrow \Pi$ are defined respectively as follows:

  \begin{align}
      X \oplus Z &\mapsto \big[[\min(X_s \cup Z_s), \max(X_s \cup Z_s)]\big]_{s \in \Sigma}\\ X \otimes Z &\mapsto \big[[\min(X_s) + \min(Z_s), \max(X_s) + \max(Z_s)]\big]_{s \in \Sigma}
  \end{align}

  To obtain the Parikh interval for a CFG up to finite length bounds, we abstractly parse the string $(\_)^n$ for all $n \in [1, l_{\max}]$ and take the union of the intervals across $[|\err\sigma| - d_{\max}, |\err\sigma| + d_{\max}]$, which subsumes every valid repair in the Levenshtein ball.

  $\mathbb{T}_2$ also allows us to sample whole parse trees by obtaining $(\Lambda_\sigma^* \circ S): \mathbb{T}_2$. Given a probabilistic CFG whose productions indexed by each nonterminal are decorated with a probability vector $\mathbf{p}$ (this may be uniform in the non-probabilistic case), we define a tree sampler $\Gamma: (\mathbb{T}_2 \mid \mathbb{T}_2^2) \rightsquigarrow \mathbb{T}$ which recursively samples children according to a Multinoulli distribution:

\begin{equation}
  \Gamma(T) \mapsto \begin{cases}
        \texttt{BTree}\Big(\texttt{root}(T), \Gamma\big(\text{Multi}(\texttt{children}(T), \mathbf{p})\big)\Big) & \text{ if $T: \mathbb{T}_2$ } \\
        \big\langle \Gamma\big(\pi_1(T)\big), \Gamma\big(\pi_2(T)\big) \big\rangle & \text{ if $T: \mathbb{T}_2\times\mathbb{T}_2$ }
  \end{cases}
\end{equation}

This is closely related to the generating function for the ordinary Boltzmann sampler from analytic combinatorics,

\begin{equation}
  \Gamma C(x) \mapsto \begin{cases}
  \text{Bern} \left(\frac{A(x)}{A(x) + B(x)}\right) \rightarrow \Gamma A(x) \mid \Gamma B(x) & \text{ if } \mathcal{C}=\mathcal{A}+\mathcal{B} \\
  \big\langle \Gamma A(x), \Gamma B(x)\big\rangle & \text{ if } \mathcal{C}=\mathcal{A} \times \mathcal{B}
  \end{cases}
\end{equation}

\noindent however unlike Duchon et al.~\cite{duchon2004boltzmann}, our work does not depend on rejection to guarantee exact-size sampling, as all trees contained in $\mathbb{T}_2$ will necessarily be the same width.


The number of binary trees inhabiting a single instance of $\mathbb{T}_2$ is sensititive to the number of nonterminals and rule expansions in the grammar. To obtain the total number of trees with breadth $n$, we abstractly parse the porous string $\sigma = (\_)^n$, letting $T=\Lambda_{\sigma}^* \circ S$, then use the recurrence below to compute the total number of trees:

\begin{equation}
  |T: \mathbb{T}_2| \mapsto \begin{cases}
%    \big|\{s \mid \big(\texttt{root}(T) \rightarrow s\big) \in P^\cap\}\big| & \text{if $T$ is a leaf,} \\
    1 & \text{if $T$ is a leaf,} \\
    \sum_{\langle T_1, T_2\rangle \in \texttt{children}(T)} |T_1| \cdot |T_2| & \text{otherwise.}
  \end{cases}
\end{equation}

To sample all trees in a given $T: \mathbb{T}_2$ uniformly without replacement, we then construct a modular pairing function $\varphi: \mathbb{T}_2 \rightarrow \mathbb{Z}_{|T|} \rightarrow \texttt{BTree}$, that we define as follows:

\begin{equation}
  \varphi(T: \mathbb{T}_2, i: \mathbb{Z}_{|T|}) \mapsto \begin{cases}
  \Big\langle\texttt{BTree}\big(\texttt{root}(T)\big), i\Big\rangle & \text{if $T$ is a leaf,} \vspace{5pt}\\
  \text{Let } b = |\texttt{children}(T)|,\\
  \phantom{\text{Let }} q_1, r=\big\langle\lfloor\frac{i}{b}\rfloor, i \pmod{b}\big\rangle,\\
  \phantom{\text{Let }} lb, rb = \texttt{children}[r],\\
  \phantom{\text{Let }} T_1, q_2 = \varphi(lb, q_1),\\
  \phantom{\text{Let }} T_2, q_3 = \varphi(rb, q_2) \text{ in } \\
  \Big\langle\texttt{BTree}\big(\texttt{root}(T), T_1, T_2\big), q_3\Big\rangle & \text{otherwise.} \\
  \end{cases}
\end{equation}

If the language is suffiently small, instead of sampling trees, we can sample integers uniformly without replacement from $\mathbb{Z}_{|T|}$ then decode them into trees using $\varphi$. This procedure is the basis for our sampling algorithm and the method we use to decode repairs from the intersection grammar.

  \subsection{Ranked Repair}\label{sec:ranking}

  Returning to the ranked repair problem (Def.~\ref{def:ranked-repair}), the above procedure returns a set of strings, and we need an ordering over them. We note that any metric is sufficient, such as the perplexity of the string under a large language model, or the probability of the string under a PCFG. We the simplest possible solution: the negative log likelihood of the string computed by a variable-order Markov chain. This is a simple, fast, and effective metric for ranking strings, and as we will show, already yields competitive results in practice.

  \section{Evaluation}

  We consider the following research questions for our evaluation:

  \begin{itemize}
    \item \textbf{RQ 1}: What properties do natural repairs exhibit? (e.g., error frequency, edit distance)
    \item \textbf{RQ 2}: How precise is our technique at fixing syntax errors? (Precision@k vs. Seq2Parse)
    \item \textbf{RQ 3}: Which design choices are most significant? (e.g., search vs. sampling, n-gram order)
  \end{itemize}

  \subsection{Dataset}

  For our evaluation, we use the StackOverflow dataset from Hindle et al.~\cite{hindle2012naturalness}. We preprocess the dataset to lexicalize both the broken and fixed code snippets, then filter the dataset by length and edit distance, in which all Python snippets whose broken form is fewer than 80 lexical tokens and whose human fix is under four Levenshtein edits is retained.

  In the following experiments, we use two datasets of Python snippets. The first is a set of 5,600 pairwise-aligned (broken, fixed) Python code snippets from Wong et al.'s StackOverflow dataset~\cite{wong2019syntax} shorter than 40 lexical tokens, whose patch sizes are less than five lexical tokens ($|\Sigma| = 50, |\err{\sigma}| \leq 40, \Delta(\err{\sigma}, \ell) \leq 4$).

  The StackOverflow dataset is comprised of 500k Python code snippets, each of which has been annotated with a human repair. We depict the normalized edit loations relative to the snippet length below.

  \begin{figure}[h!]
    \begin{tikzpicture}
      \begin{axis}[
      ybar,
      bar width=5pt,
      xlabel={Length},
      ylabel={Frequency},
      title={Snippet length},
      axis x line*=bottom,
      axis y line*=left,
      ymin=0,
      ymax=65,
      xtick=data,
      xticklabels={,\leq 20,,\leq 40,,\leq 60,,\leq 80,,\leq 100},
      ymajorgrids=true,
      grid style=dashed,
      width=0.45\textwidth,
      height=0.3\textwidth
      ]

      \addplot table {
        X Y
        1 7.60
        2 14.52
        3 22.01
        4 30.54
        5 37.82
        6 44.30
        7 49.68
        8 55.21
        9 59.75
        10 63.59
      };
      \draw[red, dashed] (axis cs:8.5,0) -- (axis cs:8.5,65);
      \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}
      \begin{axis}[
        ybar,
        bar width=5pt,
        title={Human repair distance},
        xlabel={$\Delta(\err\sigma, \sigma)$},
        ylabel={Frequency},
        axis x line*=bottom,
        axis y line*=left,
        xtick=data,
        ymajorgrids=true,
        grid style=dashed,
        xticklabels={,\leq 2,,\leq 4,,\leq 6,,\leq 8,,\leq 10},
        width=0.45\textwidth,
        height=0.3\textwidth
      ]
        \addplot table {
          X Y
          1  31.48
          2  47.52
          3  54.89
          4  60.44
          5  63.88
          6  66.38
          7  68.02
          8  70.04
          9  71.49
          10 72.22
        };
      \draw[red, dashed] (axis cs:4.5,0) -- (axis cs:4.5,80);
      \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}
      \begin{axis}[
      ybar,
      bar width=5pt,
      xlabel={Beginning $\longleftrightarrow$ End},
      ylabel={Frequency},
      title={Normalized edit locations},
      axis x line*=bottom,
      axis y line*=left,
      ymin=0,
      ymax=35,
      xtick=data,
      xticklabels={,20\%,,40\%,,60\%,,80\%,,100\%},
      ymajorgrids=true,
      grid style=dashed,
      width=0.45\textwidth,
      height=0.3\textwidth
      ]

      \addplot table {
        X Y
        10 11.6539
        20 5.7252
        30 6.2087
        40 5.9542
        50 5.5980
        60 7.9389
        70 7.0738
        80 6.9466
        90 12.4173
        100 30.4835
      };
      \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}
      \begin{axis}[
        ybar,
        bar width=5pt,
        title={Intra-patch edit distance},
        xlabel={Caret distance},
        ylabel={Frequency},
        axis x line*=bottom,
        axis y line*=left,
        xtick=data,
        ymajorgrids=true,
        grid style=dashed,
        xticklabels={1,2,3,4,5,6,7,8,9,10+},
        width=0.45\textwidth,
        height=0.3\textwidth
      ]

        \addplot table {
          X Y
          1 40.66
          2 15.00
          3 5.80
          4 4.86
          5 4.26
          6 2.98
          7 2.05
          8 2.73
          9 1.62
          10 13.64
        };
      \end{axis}
    \end{tikzpicture}
    \caption{Repair statistics across the StackOverflow dataset, of which our method can handle about half in $\sim$30s and $\sim$140GB. Larger repairs and Levenshtein radii are possible, albeit requiring additional time and memory.}\label{fig:patch_stats}
  \end{figure}

  In the second set of experiments, we use a dataset of valid Python snippets from BIFI~\cite{yasunaga2021break}, then synthetically corrupt them by introducing syntax errors sampled from a distribution trained from the StackOverflow dataset. We then measure the precision@k of our repair procedure at recovering the original, uncorrupted snippet.

  \subsection{Experimental Setup}

  We measure the precision@k of our repair procedure at recovering human repairs of varying edit distances and latency cutoffs, where both the broken and fixed code are written by a human.

  To train the scoring function, we use a length-5 variable-order Markov (VOM) chain implemented using a count-min sketch based on Apache Datasketches~\cite{apache2022datasketches}. Training on 55 million StackOverflow tokens took roughly 10 minutes, after which calculating perplexity is nearly instantaneous. Sequences are scored using negative log likelihood with Laplace smoothing and our evaluation measures the precision@\{1, 5, 10, All\} for samples at varying latency cutoffs.

  Both sets of experiments were conducted on 40 Intel Skylake cores running at 2.4 GHz, with 16 GB of RAM, running bytecode compiled for JVM 17.0.2. We measure the precision using abstract lexical matching, following the Seq2Parse~\cite{sakkas2022seq2parse} evaluation, and give a baseline for their approach on the same dataset.

  \subsection{StackOverflow Evaluation}

  For our first experiment, we run the sampler until the human repair is detected, then measure the number of samples required to draw the exact human repair across varying Levenshtein radii.

  \begin{figure}[h!]
  \input{sample_efficiency}
    \caption{Sample efficiency of LBH sampler at varying Levenshtein radii. After drawing up to $\sim10^5$ samples without replacement we can usually saturate the admissible set for almost all repairs fewer than four edits.}\label{fig:sample_efficiency}
  \end{figure}

  Next, measure the precision at various ranking cutoffs for varying wall-clock timeouts. Here, P@\{k=1, 5, 10, All\} indicates the percentage of syntax errors with a human repair of $\Delta=\{1, 2, 3, 4\}$ edits found in $\leq p$ seconds that were matched within the top-k results, based on n-gram likelihood.

  \begin{figure}[h!]
%    \resizebox{.19\textwidth}{!}{\input{bar_hillel_repair.tex}}
    \resizebox{.24\textwidth}{!}{\input{bar_hillel_repair_1}}
    \resizebox{.24\textwidth}{!}{\input{bar_hillel_repair_2}}
    \resizebox{.24\textwidth}{!}{\input{bar_hillel_repair_3}}
    \resizebox{.24\textwidth}{!}{\input{bar_hillel_repair_4}}
%    \resizebox{.24\textwidth}{!}{\input{bar_hillel_repair_5}}
%\resizebox{.3\textwidth}{!}{\input{repair1_plot.tex}}
%\resizebox{.307\textwidth}{!}{\input{repair2_plot.tex}}
    \caption{Human repair benchmark. Note the y-axis across different edit distance plots has varying ranges.}\label{fig:human}
  \end{figure}

%  In the following benchmark, we measure the precision@k of our repair procedure against human repairs of varying edit distances and latency cutoffs, using an adaptive resampling procedure described in \S\ref{sec:adaptive}. This sampler maintains a buffer of successful repairs ranked by perplexity and uses stochastic local search to resample edits within a neighborhood. Initially, edits are sampled uniformly at random. Over time and as the admissible set grows, it prioritizes edits nearby low-perplexity repairs. This technique offers a significant advantage in the low-latency setting.

  Previously, we used a rejection-based sampler, which did not sample directly from the admissible set, but the entire Levenshtein ball, then rejected invalid samples. Although rejection sampling has a much lower minimum latency threshold, the average time required to attain a fixed precision is much higher. We present the results from the earlier evaluation for comparison below.

  \begin{figure}[H]
    \resizebox{.24\textwidth}{!}{\input{repair1-3_10s_plot}}
    \resizebox{.25\textwidth}{!}{\input{repair1_10s_plot}}
    \resizebox{.24\textwidth}{!}{\input{repair2_10s_plot}}
    \resizebox{.24\textwidth}{!}{\input{repair3_10s_plot}}
    \caption{Adaptive sampling repairs. The red line indicates Seq2Parse precision@1 on the same dataset. Since it only supports generating one repair, we do not report precision@k or the intermediate latency cutoffs.}\label{fig:adaptive}
  \end{figure}

  We also evaluate Seq2Parse on the same dataset. Seq2Parse only supports precision@1 repairs, and so we only report Seq2parse precision@1 from the StackOverflow benchmark for comparison. Unlike our approach which only produces syntactically correct repairs, Seq2Parse also produces syntactically incorrect repairs and so we report the percentage of repairs matching the human repair for both our method and Seq2Parse. Seq2Parse latency varies depending on the length of the repair, averaging 1.5s for $\Delta=1$ to 2.7s for $\Delta=3$, across the entire StackOverflow dataset.

%  While adaptive sampling is able to saturate the admissible set for 1- and 2-edit repairs before the timeout elapses, 3-edit throughput is heavily constrained by compute around 16 lexical tokens, when Python's Levenshtein ball has a volume of roughly $6\times 10^8$ edits. This bottleneck can be relaxed with a longer timeout or additional CPU cores. Despite the high computational cost of sampling multi-edit repairs, our precision@all remains competitive with the Seq2Parse neurosymbolic baseline at the same latency. We provide some qualitative examples of repairs in Table~\ref{sec:appendix}.

  \begin{wrapfigure}{r}{0.35\textwidth}
    \vspace{-10pt}
    \resizebox{.35\textwidth}{!}{\input{throughput}}
    \caption{Total repairs found in 30s.}
    \label{fig:throughput}
    \vspace{-10pt}
  \end{wrapfigure}

  End-to-end throughput varies significantly with the edit distance of the repair. Some errors are trivial to fix, while others require a large number of edits to be sampled before a syntactically valid edit is discovered. We evaluate throughput by sampling edits across invalid strings $|\err\sigma| \leq 40$ from the StackOverflow dataset of varying length, and measure the total number of syntactically valid edits discovered, as a function of string length and language edit distance $\Delta\in[1, 3]$. Each trial is terminated after 10 seconds, and the experiment is repeated across 7.3k total repairs. Note the y-axis is log-scaled, as the number of admissible repairs increases sharply with language edit distance. Our approach discovers a large number of syntactically valid repairs in a relatively short amount of time, and is able to quickly saturate the admissible set for $\Delta(\err\sigma, \sigma) \in [1, 4]$ before timeout. As Seq2Parse only generates one syntactically valid repair per string, we do not report its throughput.

  \begin{wrapfigure}{l}{0.35\textwidth}
    \resizebox{.35\textwidth}{!}{\input{experiments/timings}}
    \caption{End-to-end repair timings across snippet length.}
    \label{fig:timings}
    \vspace{-20pt}
  \end{wrapfigure}

  In Fig.~\ref{fig:timings}, we plot the end-to-end repair timings across snippet length. To measure the timings, we collect 1000 repair samples of varying lengths and edit distances, then measure the time until the sampler locates the exact human repair. For each code snippet, we measure the total time taken, for repairs between 1-4 Levenshtein edits. Note the logarithmic scale on the y-axis.

  \subsection{Synthetic repair benchmark}\label{sec:latency}

  Pairwise naturally-occurring errors and human fixes are the most authentic source of real-world syntax repairs, but can be difficult to obtain due to the paucity of parallel syntax error corpi. In the absence of natural syntax repairs, one viable alternative is to collect a dataset of syntactically valid code, and synthetically corrupt it. The original source code becomes the ground truth repair for the synthetically generated typo, and the target for evaluating the precision of our repair procedure.

  In our synthetic repair benchmark, we synthetically corrupt Python code snippets using a learned corruption model. To train the model, we compute the Levenshtein alignment on the StackOverflow dataset, then approximate the conditional probability of each edit given the local context. During evaluation, we sample a corruption from the learned typo distribution, and measure the precision of our model at recovering the originally valid lexical sequence.

  Suppose we have a dataset of Levenshtein edits and their local context. For simplicity, we shall assume a trigram language model, i.e., $P(\sigma_i' \mid \sigma_{i-1}, \sigma_i, \sigma_{i+1})$, however the approach can be generalized to higher-order Markov models. Given a string $\sigma$, we can sample error trajectories $q^1(\sigma), q^2(\sigma), \ldots, q^n(\sigma)$ by defining $q(\sigma)$ to sample a single edit from the set of all relevant edit actions $Q(\sigma)$, then recursively applying $q$ to the resulting string. More formally,

  \begin{enumerate}
    \item Given a string $\sigma$, compute $Q(\sigma)$, the set of all relevant edit actions for all possible edit locations by unioning the set of all possible edits at each location, i.e., $Q(\sigma) \coloneqq \bigcup_{i=1}^{|\sigma| - 1} \big\{\sigma_i' \mid  0 < P(\sigma_i \mid \sigma_{i-1}, \sigma_i, \sigma_{i+1})\big\}$.
    \item Renormalize the probabilities of each edit $P(q \mid \sigma)$ by $\sum_{q \in Q(\sigma)} P(q)$. This ensures the probability of sampling a particular edit is proportional to its relative probability under the language model and sums to 1.
    \item Sample an edit $q(\sigma) \sim Q(\sigma)$, then repeat for $n$ steps where $n$ is sampled from a geometric distribution with mean $\mu$ matching the average edit distance of the dataset (this assumes the edit distance is independent of the edits).
  \end{enumerate}

  For example, suppose we have the following patch in our initial dataset:\\

  \texttt{BOS \hlred{def} NAME ( NAME ) : NEWLINE \hlred{INDENT} return \hlorange{NAME} NEWLINE \hlgreen{INDENT} NEWLINE EOS}\\

  From this patch, the following contextual typo probabilities will be incremented:

  \begin{align*}
    P(\texttt{BOS \hlred{def} NAME}) &\mathrel{+}= 1 &P(\texttt{NEWLINE \hlred{INDENT} return}) &\mathrel{+}= 1\\
    P(\texttt{return \hlorange{NAME} NEWLINE}) &\mathrel{+}= 1 & P(\texttt{NEWLINE \hlgreen{INDENT} NEWLINE}) &\mathrel{+}= 1
  \end{align*}

  Later, these contextual probabilities will allow us to sample a synthetic corruption matching the distribution of typos in the dataset. We then measure the precision at recovering the originally valid string.

  Here, we conduct an ablation study to compare the effectiveness of PCFG sampling vs. the enumerative sampler. In both experiments, we run the corresponding sampler for 30 seconds (either enumerative or PCFG), then rerank all repairs by n-gram perplexity and measure the Precision@1 for each each across varying edit distances.

  \begin{figure}[h]
    \input{experiments/ablation_enumeration_only.tex}
    \input{experiments/ablation_pcfg_only.tex}
  \end{figure}

  While the overall precision is notably lower for PCFG sampling than enumeration, the average number of samples drawn is also significantly lower, indicating a relatively higher sample efficiency. This illustrates the tradeoff between sample efficiency and diversity, and suggests that a hybrid approach may be the most effective. When the repair language is very large, a PCFG offers a more informed prior, albeit at the cost of lower coverage.

  In general enumeration has an advantage when the CFL is small. For example, if the CFL contains 2,000 sentences, enumeration will recover all 2,000, whereas PCFG sampling may only recover 100 of the most likely samples. However, if the CFL has 200,000 sentences, enumeration may only be able to recover 10,000 uniform random samples and the PCFG may only recover 5,000, but due to the higher sample efficiency, the PCFG samples are more likely to contain the human repair.

  \section{Related Work}\label{sec:related}

  Three important questions arise when repairing syntax errors: (1) is the program broken in the first place? (2) if so, where are the errors located? (3) how should those locations then be altered? In the case of syntax correction, those questions are addressed by three related research areas, (1) parsing, (2) language equations and (3) repair. We survey each of those areas in turn.

  \subsection{Parsing}

  Context-free language (CFL) parsing is the well-studied problem of how to turn a string into a unique tree, with many different algorithms and implementations (e.g., shift-reduce, recursive-descent, LR). Many of those algorithms expect grammars to be expressed in a certain form (e.g., left- or right- recursive) or are optimized for a narrow class of grammars (e.g., regular, linear).

  General CFL parsing allows ambiguity (non-unique trees) and can be formulated as a dynamic programming problem, as shown by Cocke-Younger-Kasami (CYK)~\cite{sakai1961syntax}, Earley~\cite{earley1970efficient} and others. These parsers have roughly cubic complexity with respect to the length of the input string.

  As shown by Valiant~\cite{valiant1975general}, Lee~\cite{lee2002fast} and others, general CFL recognition is in some sense equivalent to binary matrix multiplication, another well-studied combinatorial problem with broad applications, known to be at worst subcubic. This reduction unlocks the door to a wide range of complexity-theoretic and practical speedups to CFL recognition and fast general parsing algorithms.

  Okhotin (2001)~\cite{okhotin2001conjunctive} extends CFGs with language conjunction in \textit{conjunctive grammars}, followed by Zhang \& Su (2017)~\cite{zhang2017context} who apply conjunctive language reachability to dataflow analysis.

  \subsection{Language equations}

  Language equations are a powerful tool for reasoning about formal languages and their inhabitants. First proposed by Ginsburg et al.~\cite{ginsburg1962two} for the ALGOL language, language equations are essentially systems of inequalities with variables representing \textit{holes}, i.e., unknown values, in the language or grammar. Solutions to these equations can be obtained using various fixpoint techniques, yielding members of the language. This insight reveals the true algebraic nature of CFLs and their cousins.

  Being an algebraic formalism, language equations naturally give rise to a kind of calculus, vaguely reminiscent of Leibniz' and Newton's. First studied by Brzozowski~\cite{brzozowski1964derivatives, brzozowski1980equations} and Antimirov~\cite{antimirov1996partial}, one can take the derivative of a language equation, yielding another equation. This can be interpreted as a kind of continuation or language quotient, revealing the suffixes that complete a given prefix. This technique leads to an elegant family of algorithms for incremental parsing~\cite{might2011parsing, adams2016complexity} and automata minimization~\cite{brzozowski1962canonical}. In our setting, differentiation corresponds to code completion.

  In this paper, we restrict our attention to language equations over context-free and weakly context-sensitive languages, whose variables coincide with edit locations in the source code of a computer program, and solutions correspond to syntax repairs. Although prior work has studied the use of language equations for parsing~\cite{might2011parsing}, to our knowledge they have never previously been considered for the purpose of code completion or syntax error correction.

%Rosenkrantz~\cite{rosenkrantz1967matrix} introduces , and

%When a sentence contains so-called \textit{holes}, parsing becomes slightly more challenging. Holes are special tokens that can be filled by words in the grammar. A sentence might have multiple holes, representing a set of contextually valid words. For example, ``\textit{\_ \_ to eat \_}'' which could be filled by, e.g., ``\textit{I like\ldots sushi}'', ``\textit{They want\ldots pizza}'', or ``\textit{We need\ldots something}''.

  \subsection{Syntax repair}

  In finite languages, syntax repair corresponds to spelling correction, a more restrictive and largely solved problem. Schulz and Stoyan~\cite{schulz2002fast} construct a finite automaton that returns the nearest dictionary entry by Levenshtein edit distance. Though considerably simpler than syntax correction, their work shares similar challenges and offers insights for handling more general repair scenarios.

  When a sentence is grammatically invalid, parsing grows more challenging. Like spelling, the problem is to find the minimum number of edits required to transform an arbitrary string into a syntactically valid one, where validity is defined as containment in a (typically) context-free language. Early work, including Irons~\cite{irons1963error} and Aho~\cite{aho1972minimum} propose a dynamic programming algorithm to compute the minimum number of edits required to fix an invalid string. Prior work on error correcting parsing only considers the shortest edit(s), and does not study multiple edits over the Levenshtein ball. Furthermore, the problem of actually generating the repairs is not well-posed, as there are usually many valid strings that can be obtained within a given number of edits. We instead focus on bounded Levenshtein reachability, which is the problem of finding useful repairs within a fixed Levenshtein distance of the broken string, which requires language intersection.

%Bar-Hillel~\cite{bar1961formal} establishes the closure of CFLs under intersection with regular languages, and can be used to construct the corresponding minimum-cost CFG in a straightforward manner. However while generally quite expressive, CFLs are themselves not closed under intersection and have other practical limitations, i.e., are unable to express indentation or variable binding. These limitations motivate us towards more expressive yet still efficiently parsable formalisms.

%Conveniently, Okhotin~\cite{okhotin2001conjunctive} offers exactly the formalism needed: language equations augmented with logical operators like conjunction or disjunction. These operators afford us the flexibility to encode both language union and intersection, which are difficult or impossible to express using a single grammar, as well as incorporate extra-syntactic side-constraints during solving.

%Naively, one might think to enumerate all edits within a certain Levenshtein radius and reject invalid strings. However, this approach is intractable for long strings. Another approach is to compute the \textit{edit distance} between the string and the language as proposed by Aho. This approach is also intractable depending on the size of the grammar. A third approach is to use a language equation to compute the intersection between the Levenshtein hypersphere and the language.

%The literature on parsers is vast and deep, covering far more ground than we could possibly hope to survey. We take inspiration from their findings, but restrict ourselves to a dozen most closely related papers, in four major research areas: (1) formal language theory, (2) constraint satisfaction (3) program synthesis, and (4) error correction. We survey each of these areas in turn.
%
%It was Noam Chomsky who first developed the algebraic theory of \textit{context-free grammars} (CFGs) in 1959~\cite{chomsky1959algebraic}, and since then, CFGs have been the subject of extensive research in formal language theory and program analysis. In particular, we take inspiration from Leslie Valiant~\cite{valiant1975general} who first discovered the connection to matrix multiplication in 1975 and Alexander Okhotin~\cite{okhotin2001conjunctive} who later introduced the idea of \textit{conjunctive grammars} in 2001. More recently, Azimov \& Grigorev~\cite{azimov2018cfpq} and Qirun Zhang shed light into the practical applicability of these ideas and made the theory of CFG reachability more accessible to the general public. In our work, we show how to compile their ideas onto a SAT solver to fix syntax errors, which seems like a perfectly natural extension, but was heretofore previously never considered to the best of our knowledge.

  \subsection{Classical program synthesis}

  There is related work on string constraint solving in the constraint programing literature, featuring solvers like CFGAnalyzer and HAMPI~\cite{kiezun2009hampi}, which consider bounded context free grammars and intersections thereof. Axelson et al. (2008)~\cite{axelsson2008analyzing} has some work on incremental SAT encoding but does not exploit the linear-algebraic structure of parsing, conjunctive reachability nor provide real-time guarantees. D'Antoni et al. (2014) introduces \textit{symbolic automata}~\cite{dantoni2014minimization}, a generalization of finite automata which allow infinite alphabets and symbolic expressions over them. In none of the constraint programming literature we surveyed do any of the approaches employ matrix-based parsing, and therefor do not enjoy the optimality guarantees of Valiant's parser. Our solver can handle context-free and conjunctive grammars with finite alphabets and does not require any special grammar encoding. The matrix encoding makes it particularly amenable to parallelization.

  \subsection{Error correcting codes}

  Our work focuses on errors arising from human factors in computer programming, in particular \textit{syntax error correction}, which is the problem of fixing partially corrupted programs. Modern research on error correction, however, can be traced back to the early days of coding theory when researchers designed \textit{error-correcting codes} (ECCs) to denoise transmission errors induced by external interference, e.g., collision with a high-energy proton, manipulation by an adversary or even typographical mistake. In this context, \textit{code} can be any logical representation for communicating information between two parties (such as a human and a computer), and an ECC is a carefully-designed scheme which ensures that even if some portion of the message should become corrupted, one can still recover the original message by solving a linear system of equations. When designing ECCs, one typically assumes a noise model over a certain event space, such as the Hamming~\cite{titsias2017hamming, dong2023number} or Levenshtein~\cite{becerra2008learning, barlev2021levenshtein} balls, from which we draw inspiration for our work.

  \subsection{Neural program repair}

  The recent success of deep learning has lead to a variety of work on neural program repair~\cite{allamanis2021self, chirkova2021empirical, drain2021generating}. These approaches typically employ Transformer-based neural language models (NLMs) and model the problem as a sequence-to-sequence transformation. Although recent work on circuit lower bounds have cast doubt on the ability of Transformers to truly learn formal languages~\cite{merrill2022saturated, chiang2023tighter}, expressivity aside, these models have been widely adopted for practical program repair tasks. In particular, two papers stand out being most closely related to our own: Break-It-Fix-It (BIFI)~\cite{yasunaga2021break} and Seq2Parse~\cite{sakkas2022seq2parse}. BIFI adapts techniques from semi-supervised machine translation to generate synthetic errors in clean code and fixes them. This reduces the amount of pairwise training data, but may generalize poorly to natural errors. Seq2Parse combines a transformer-based model with an augmented version of the Early parser to suggest error rules, but only suggests a single repair. Our work differs from both in that we suggest multiple repairs with much lower latency, do not require a pairwise repair dataset, and can fix syntax errors in any language with a well-defined grammar. We note our approach is complementary to existing work in neural program repair, and may be used to generate synthetic repairs for training or employ a NLM model to rank its repairs.

  \section{Discussion}\label{sec:discussion}

  The main lesson we draw from our experiments is that it is possible to leverage compute to compete with large language models on practical program repair tasks. Though extremely sample-efficient, their size comes at the cost of higher latency, and domain adaptation requires fine-tuning or retraining on pairwise repairs. Our approach uses a tiny grammar and a relatively cheap ranking metric to achieve comparable accuracy at the same latency. This allows us to repair errors in languages with little to no training data and provides far more flexibility and controllability.

  Our primary insight leading to SoTA precision@\{5,10\} is that repairs are typically concentrated near a small number of edit locations, and by biasing the search toward previously successful edit locations, we can achieve a significant speedup over a na\"ive search. We note this heuristic may not be applicable to all grammars, and it may be possible to construct less natural counterexamples where this heuristic fails, although we consider these unlikely in practice.

  Latency can vary depending on many factors including string length and grammar size, and critically the Levenshtein edit distance. This can be an advantage because in the absence of any contextual or statistical information, syntax and Levenshtein edits are often sufficiently constrained to determine a small number of valid repairs. It is also a limitation because as the number of edits grows, the admissible set grows rapidly and the number of valid repairs may become too large to be useful without a good metric, depending on the language and source code snippet under repair.

  Tidyparse in its current form has several other technical shortcomings: firstly, it does not incorporate any neural language modeling technology at present, an omission we hope to address. Training a language model to predict likely repair locations and rank admissible results could lead to lower overall latency and more natural repairs. We also hope to explore the use of Metropolis-Hastings and determinantal point processes to encourage sampling diversity.

  Secondly, our current method does not specialize language intersection to the grammar family, nor employ Bar-Hillel's~\cite{bar1961formal} construction for REG-CFL intersection, which would lead to a more efficient encoding of Levenshtein-CFL reachability. Furthermore, considering recent extensions of Boolean matrix-based parsing to linear context-free rewriting systems (LCFRS)~\cite{cohen2016parsing}, it may be feasible to search through richer language families within the SAT solver without employing an external stochastic search to generate and validate candidate repairs.

  Lastly and perhaps most significantly, Tidyparse does not incorporate any semantic constraints, so its repairs whilst syntactically admissible, are not guaranteed to be semantically valid. This can be partly alleviated by filtering the results through an incremental compiler or linter, however, the latency introduced may be non-negligible. It is also possible to encode type-based semantic constraints into the solver and we intend to explore this direction more fully in future work.

  We envision a few primary use cases for our tool: (1) helping novice programmers become more quickly familiar with a new programming language, (2) autocorrecting common typos among proficient but forgetful programmers, (3) as a prototyping tool for PL designers and educators, and (4) as a pluggable library or service for parser-generators and language servers. Featuring a grammar editor and built-in SAT solver, Tidyparse helps developers navigate the language design space, visualize syntax trees, debug parsing errors and quickly generate simple examples and counterexamples for benchmarking and testing.

  \section{Conclusion}\label{sec:conclusion}

  Our work, while a case study on syntax repair, is part of a broader trend in program synthesis that investigates how to combine the strengths of formal language theory and machine learning to build more powerful and flexible programming tools. One approach is to filter the outputs of a generative language model to satisfy a formal specification, typically by constrained sampling. Alternatively, some attempt to use a formal language to guide the search for valid programs via a reinforcement learning or hybrid neurosymbolic approach.

  In our work, we take a more pragmatic tack - by incorporating the similarity metric into our formal language, we try to exhaustively enumerate repairs by increasing distance, then use the stochastic language model to rank the resulting solutions by naturalness. The more constraints we can encode into the formal language, the more efficient sampling becomes, and the more precise control we have over the output. This reduces the need for training a large, expensive language model to relearn syntax, and allows us to leverage compute for more efficient search.

  The great compromise in program synthesis is one of efficiency versus expressiveness. The more expressive a language, the more concise and varied the programs it can represent, but the harder those programs are to synthesize without resorting to domain-specific heuristics. Likewise, the simpler a language is to synthesize, the weaker its concision and expressive power.

  Most existing work on program synthesis has focused on general $\lambda$-calculi, or narrow languages such as finite sets or regular expressions. The former are too expressive to be efficiently synthesized or verified, whilst the latter are too restrictive to be useful. In our work, we focus on context-free and mildly context-sensitive grammars, which are expressive enough to capture a variety of useful programming language features, but not so expressive as to be unsynthesizable.

  The second great compromise in program synthesis is that of reusability versus specialization. In programming, as in human communications, there is a vast constellation of languages, each requiring specialized generators and interpreters. Are these languages truly irreconcilable? Or, as Noam Chomsky argues, are these merely dialects of a universal language? \textit{Synthesis} then, might be a misnomer, and more aptly called \textit{recognition}, in the analytic tradition.

  In our work, we argue these two compromises are not mutually exclusive, but complementary and reciprocal. Programs and the languages they inhabit are indeed synthetic, but can be analyzed and reused in the metalanguage of context-free grammars closed under conjunction. Not only does this admit an efficient synthesis algorithm, but allows users to introduce additional constraints without breaking compositionality, one of the most sacred tenets in programming language design.

  Over the last few years, there has been a surge of progress in applying language models to write programs. That work is primarily based on methods from differential calculus and continuous optimization, leading to the so-called \textit{naturalness hypothesis}, which suggests programming languages are not so different from natural ones. In contrast, programming language theory takes the view that languages are essentially discrete and finitely-generated sets, although perhaps uncomputably large ones, governed by logical calculi. Programming, thus viewed, is more like a mathematical exercise in constraint satisfaction, an oft-overlooked but unavoidable aspect of translating abstract ideas into computing machinery. These constraints naturally arise at various stages of syntax validation, type-checking and runtime verification, and help to ensure programs fulfill their intended purpose.

  As our work shows, not only is linear algebra over finite fields an expressive language for probabilistic inference, but also an efficient framework for inference on languages themselves. Borrowing analysis techniques from multilinear algebra and tensor completion in the machine learning setting, we develop an equational theory that allows us to translate various decision problems on formal languages into a system of inequalities over finite fields. As a practical consequence, this means we can efficiently encode a number of problems in parsing, code completion and program repair using SAT solvers, which are known to be highly efficient, scalable and flexible to domain-specific constraints. We demonstrate the effectiveness of our approach in a variety of practical scenarios, including code completion and program repair for linear conjunctive languages, and show that our approach is competitive with state-of-the-art methods in terms of both accuracy and efficiency. In contrast with LL and LR-style parsers, our technique can recover partial forests from invalid strings, and handle arbitrary conjunctive languages. In future work, we hope to extend our method to more natural grammars like PCFG, LCFRS and other mildly context-sensitive languages.

  Syntax correction tools should be as user-friendly and widely-accessible as autocorrection tools in word processors. From a practical standpoint, we argue it is possible to reduce disruption from manual syntax repair and improve the efficiency of working programmers by driving down the latency needed to synthesize an acceptable repair. In contrast with program synthesizers that require intermediate editor states to be well-formed, our synthesizer does not impose any constraints on the code itself being written and can be used in a live programming environment.

  Despite its computational complexity, the design of the tool itself is relatively simple. Tidyparse accepts a linear conjunctive language and a string. If the string is valid, it returns the parse forest, otherwise, it returns a set of repairs, ordered by their perplexity. Tidyparse compiles the grammar and candidate string onto a discrete dynamical system using an extended version of Valiant's algorithm and solves for its fixedpoints using an incremental SAT solver. By allowing the string to contain holes, repairs may contain either concrete tokens or nonterminals, which can be expanded by the user or a neural-guided search procedure to produce a valid program. This approach to parsing has many advantages, enabling us to repair syntax errors, correct typos and recover from errors in real time, as well as being provably sound and complete with respect to the grammatical specification. It is also compatible with neural program synthesis and repair techniques and shares the same masked language modeling (MLM) target used by Transformer-based LLMs.

  We have implemented our approach as an IDE plugin and demonstrated its viability as a practical tool for realtime programming. A considerable amount of effort was devoted to supporting fast error correction functionality. Tidyparse is capable of generating repairs for invalid code in a range of practical languages with very little downstream language integration required. We plan to continue expanding its grammar and autocorrection functionality to cover a broader range of languages and hope to conduct a more thorough user study to validate its effectiveness.

%\subsection{Ranking}
%
%Since the number of solutions can be very large, we can use a language model to rank the results maximizing likelihood, or minimizing perplexity, subject to the constraints. This ranking can be used to guide the propagation, sample the choice function, sample hole locations or as a post-processing step after a fixed timeout has expired.

%Alternatively, this expression can be rewritten as a polynomial over GF(2):
%
%\[
%  (v_1 \times w_2 + y_3 + 1) \Leftrightarrow [S \in Y] \Leftrightarrow [Q R \in L(G)]
%\]

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
%  \bibliographystyle{splncs04}
  \bibliography{../bib/acmart}
\end{document}