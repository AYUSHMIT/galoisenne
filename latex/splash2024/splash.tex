%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[sigplan,10pt,review,anonymous]{acmart}
%\settopmatter{printfolios=false,printccs=false,printacmref=false}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
%\documentclass[sigplan,nonacm]{acmart}
\documentclass[sigplan,review,anonymous,acmsmall]{acmart}\settopmatter{printfolios=false,printccs=false,printacmref=false}


%% Conference information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
\acmConference[SPLASH'24]{ACM SIGPLAN conference on Systems, Programming, Languages, and Applications: Software for Humanity}{October 22-27, 2024}{Pasadena, California, United States}
%\acmYear{2018}
%\acmISBN{} % \acmISBN{978-x-xxxx-xxxx-x/YY/MM}
%\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
%\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2018}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{acmart}

\input{preamble}
\begin{document}
%
  \title{Syntax Repair as Language Intersection}
  %
  \begin{abstract}
    We introduce a new technique for correcting syntax errors in arbitrary context-free languages. Our work comes from the observation that syntax errors with a small repair typically have very few unique small repairs, which can usually be enumerated within a small edit distance then ranked within a short amount of time. Furthermore, we place a heavy emphasis on precision: the enumerated set must contain every possible repair within a few edits and no invalid repairs. To do so, we reduce CFL recognition onto Boolean tensor completion, then model error correction as a language intersection problem between a Levenshtein automaton and a context-free grammar. To decode the solutions, we then sample trees without replacement from the intersection grammar, which yields valid repairs within a certain Levenshtein distance. Finally, we rank all repairs discovered within 60 seconds by a Markov chain.
    \keywords{Error correction \and CFL reachability \and Langauge games.}
  \end{abstract}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
%  \author{Breandan Considine\inst{1} \and
%  Jin Guo\inst{1}\and
%  Xujie Si\inst{2}}
%
%  \authorrunning{Considine et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
%  \institute{McGill University, Montr\'eal, QC H2R 2Z4, Canada\\
%  \email{\{breandan.considine@mail, jguo@cs\}.mcgill.ca}\and
%  University of Toronto, Toronto, ON, M5S 1A1 Canada\\
%  \email{six@utoronto.ca}}
%
  \maketitle              % typeset the header of the contribution


  \section{Introduction}

  Syntax repair is the problem of modifying an invalid sentence so it conforms to some grammar. Prior work has been devoted to fixing syntax errors using handcrafted heuristics. This work features a variety of approaches including rule-based systems and statistical language models. However, these techniques are often brittle, and are susceptible to misgeneralization. In a prior paper published in SPLASH, the authors sample production rules from an error correcting grammar. While theoretically sound, this technique is incomplete, i.e., not guaranteed to sample all edits within a certain Levenshtein distance, and no more. In this paper, we demonsrate it is possible to attain a significant advantage by synthesizing and scoring all repairs within a certain Levenshtein distance. Not only does this technique guarantee perfect generalization, but also helps with precision.

  We take a first-principles approach making no assumptions about the sentence or grammar and focuses on correctness and end-to-end latency. Our technique is simple:

  \begin{enumerate}
    \item We first reduce the problem of CFL recognition to Boolean tensor completion, then use that to compute the Parikh image of the CFL. This follows from a straightforward extension of the Chomsky-Sch\"utzenberger enumeration theorem.
    \item We then model syntax correction as a language intersection problem between a Levenshtein automaton and a context-free grammar, which we explicitly materialize using a specialized version of the Bar-Hillel construction to Levenshtein intersections. This greatly reduces the number of generated productions.
    \item To decode the members from the intersection grammar, we sample trees without replacement by constructing a bijection between syntax trees and the integers, then sampling integers uniformly without replacement from a finite range. This yields concrete repairs within a certain Levenshtein distance.
    \item Finally, we rank all repairs found within 60 seconds by a Markov chain.
  \end{enumerate}

  Though simple, this technique outperforms SoTA syntax repair techniques. Its efficacy owes to the fact it does not sample edits or nor productions, but unique, fully formed repairs within a certain Levenshtein distance. It is sound and complete up to a Levenshtein bound - i.e., it will find all repairs within an arbitrary Levenshtein distance, and no more. Often the langauge of small repairs is surpisingly small compared with the langauge of all possible edits, enabling us to efficiently synthesize and score every possible solution. This offers a significant advantage over memoryless sampling techniques.

  \pagebreak

  \input{example_contents}


  \section{Problem}

  We can model syntax repair as a language intersection problem between a context-free language (CFL) and a regular language.

  \begin{definition}[Bounded Levenshtein-CFL reachability]
    Given a CFL $\ell$ and an invalid string $\err{\sigma}: \ell^\complement$, the BCFLR problem is to find every valid string reachable within $d$ edits of $\err{\sigma}$, i.e., letting $\Delta$ be the Levenshtein metric and $L(\err\sigma, d) \coloneqq \{\sigma \mid \Delta(\err{\sigma}, \sigma) \leq d\}$, we seek to find $L(\err\sigma, d) \cap \ell$.
  \end{definition}

  To solve this problem, we will first pose a simpler problem that only considers intersections with a finite language, then turn our attention back to BCFLR.

  \begin{definition}[Porous completion]
    Let $\underline\Sigma \coloneqq \Sigma \cup \{\_\}$, where $\_$ denotes a hole. We denote $\sqsubseteq: \Sigma^n \times \underline\Sigma^n$ as the relation $\{\langle\sigma', \sigma\rangle \mid \sigma_i \in \Sigma \implies \sigma_i' = \sigma_i\}$ and the set of all inhabitants $\{\sigma': \Sigma^+ \mid \sigma' \sqsubseteq \sigma\}$ as $\text{H}(\sigma)$. Given a \textit{porous string}, $\sigma: \underline\Sigma^*$ we seek all syntactically valid inhabitants, i.e., $A(\sigma)\coloneqq\text{H}(\sigma)\cap\ell$.
  \end{definition}

  As $A(\sigma)$ can be a large-cardinality set, we want a procedure which prioritizes natural solutions:

  \begin{definition}[Ranked repair]
    Given a finite language $A = L(\err\sigma, d) \cap \ell$ and a probabilistic language model $P: \Sigma^* \rightarrow [0, 1] \subset \mathbb{R}$, the ranked repair problem is to find the top-$k$ maximum likelihood repairs under the language model. That is,
    \begin{equation}
      R(A, P) \coloneqq \argmax_{\{\bm{\sigma} \mid \bm{\sigma} \subseteq A, |\bm{\sigma}| \leq k\}} \sum_{\sigma \in \bm{\sigma}}\text{P}(\sigma\mid\err\sigma, \theta)
    \end{equation}
    % On average, across all $G, \sigma$ $\hat{R}$ should approximate $R$.
%    We want a procedure $\hat{R}$, minimizing $\mathbb{E}_{G, \sigma}\big[D_{\text{KL}}(\hat{R} \parallel R)\big]$ and wallclock runtime.
  \end{definition}

  The problem of ranked repair is typically solved by learning a distribution over string edits, however this approach can be sample-inefficient and generalize poorly to new langauges. Modeling the distribution over $\Sigma^*$ forces the model to learn both syntax and stylistics. As we demonstrate, this problem can be decomposed into a bilevel objective: first retrieval, then ranking. By ensuring retrieval is sufficiently precise and exhaustive, maximizing likliehood over the set of proximal repairs can be achieved with a much weaker, syntax-oblivious language model.

  Even with an extremely efficient approximate sampler for $\sigma \sim \ell_\cap$, due to the large cardinality $A$, it would be intractable to sample either $\ell\cap\Sigma^n$ or $L(\err\sigma, d)$, then reject invalid ($\sigma \notin \ell$) or unreachable ($\sigma \notin L(\err\sigma, d)$) edits, and completely out of the question to sample $\sigma \sim \Sigma^*$ as do many large language models.

  Instead, we will explicitly construct a grammar generating $\ell \cap L(\err\sigma, d)$, sample from it, then rerank all repairs after a fixed timeout. So long as $|\ell_\cap|$ is sufficiently small and recognizes all and only small repairs, our sampler is sure to retrieve the most natural repair and terminate quickly. Then, the problem becomes one of ranking only sampled repairs which can be completed quickly using a Markov chain.

  \section{Method}

  \begin{wrapfigure}{r}{0.45\textwidth}
%\begin{figure}[h!]
    \vspace{-0.8cm}
    \begin{center}
      \resizebox{0.45\textwidth}{!}{
        \begin{tikzpicture}[node distance=2cm]
          \node (start) [startstop] {Start};
          \node (pro1) [process, below of=start, yshift=-0.5cm] {$\mathcal{G}_\cap \leftarrow \mathcal{G}\cap\Delta(\err\sigma, d)$};
          \node (pcfg) [io, left of=pro1, xshift=-3cm] {[P]CFG};
          \node (lnfa) [io, right of=pro1, xshift=3cm] {L-WFA};

          \node (code) [io, right of=start,xshift=3cm] {Code};
          \node (synt) [io, left of=start,xshift=-3cm] {Syntax};

          \node (dec1) [decision, below of=pro1, yshift=-0.5cm] {$[\mathcal{G}_{\cap} = \varnothing]$};

          \node (pro2b) [process, right of=dec1, xshift=3cm] {Increase radius, $d$};

          \node (dec2) [decision, below of=dec1, yshift=-1cm] {$|\mathcal{L}(\mathcal{G}_\cap)|$};

          \node (samp1) [process, left of=dec2, xshift=-3cm] {Enumerate $\sigma' \in \mathcal{L}(\mathcal{G}_\cap)$};
          \node (samp2) [process, right of=dec2, xshift=3cm] {Sample $\sigma' \sim P(\mathcal{G}_\cap)$};

          \node (rank) [process, below of=dec2, yshift=-2.5cm] {Rerank by $L_\theta(\sigma')$};
          \node (vlmc) [io, below of=rank, yshift=-0.1cm] {VLMC};

%  \node (out1) [io, below of=pro2a] {Output};
          \node (stop) [startstop, above of=rank] {Done};

%  \draw [arrow] (dec0) -- node[anchor=east] {no} (pro1);

          \draw [->] (0, 1.5) -- (start);

          \draw [arrow] (start) -- (code);
          \draw [arrow] (start) -- (synt);
          \draw [arrow] (code) -- (lnfa);
          \draw [arrow] (synt) -- (pcfg);
          \draw [arrow] (lnfa) -- (pro1);
          \draw [arrow] (pcfg) -- (pro1);

%  \draw [arrow] (in1) -- (pro1);
          \draw [arrow] (pro1) -- (dec1);
          \draw [arrow] (dec1) -- node[anchor=south] {yes} (pro2b);
          \draw [arrow] (dec1) -- node[anchor=east] {no} (dec2);
          \draw [arrow] (pro2b) -- (lnfa);
          \draw [arrow] (dec2) -- node[anchor=south] {small} (samp1);
          \draw [arrow] (dec2) -- node[anchor=south] {large} (samp2);

          \draw [arrow] (vlmc) -- (rank);
          \draw [arrow] (samp1) |- (rank);
          \draw [arrow] (samp2) |- (rank);
%  \draw [arrow] (pro2a) -- (out1);
          \draw [arrow] (rank) -- (stop);
          \draw [arrow] (dec2) -- node[anchor=east] {1} (stop);

        \end{tikzpicture}
      }
    \end{center}
    \caption{Flowchart of our proposed method.}\label{fig:flowchart}
    \vspace{-1.5cm}
  \end{wrapfigure}

  The syntax of most programming languages is context-free. Our proposed method is simple. We construct a context-free grammar representing the intersection between the langauge syntax and an automaton recognizing the Levenshtein ball of a given radius. Since CFLs are closed under intersection with regular languages, this is admissible. Three outcomes are possible:

  \begin{enumerate}
    \item $\mathcal{G}_\cap$ is empty, in which case there is no repair within the given radius. In this case, we simply increase the radius and try again.
    \item $\mathcal{L}(\mathcal{G}_\cap)$ is small, in which case we enumerate all possible repairs. Enumeration is tractable for $\sim 80\%$ of the dataset in $\leq 90$s.
    \item $\mathcal{L}(\mathcal{G}_\cap)$ is too large to enumerate, so we sample from the intersection grammar $\mathcal{G}_\cap$. Sampling is necessary for $\sim20\%$ of the dataset.
  \end{enumerate}

  When ambiguous, we use an n-gram model to rank and return the top-k results by likelihood. This procedure is depicted in Fig.~\ref{fig:flowchart}.

  \pagebreak\subsection{The Nominal Levenshtein Automaton}

  \begin{wrapfigure}{r}{0.5\textwidth}
    \vspace{-0.3cm}
    \begin{center}
      \input{nfa_cfg.tex}
    \end{center}
    \caption{NFA recognizing Levenshtein $\Delta(\sigma: \Sigma^5, 3)$.}\label{fig:lev_nfa}
    \vspace{-0.5cm}
  \end{wrapfigure}

  Levenshtein edits are recognized by a certain kind of automaton, known as the Levenshtein automaton. Since the original approach used by Schultz and Mihov contains cycles and epsilon transitions, we propose a modified variant which is epsilon-free, acyclic and monotone. Furthermore, we use a nominal automaton, allowing for infinite alphabets. This considerably simplifies the langauge intersection. We give an example of a small Levenshtein automaton recognizing $\Delta(\sigma: \Sigma^5, 3)$ in Fig.~\ref{fig:lev_nfa}. Unlabeled arcs accept any terminal.

  \noindent Alternatively, this transition system can be viewed as a kind of proof system. This is equivalent to the Levenshtein automaton used by Schultz and Mihov, but is more amenable to our purposes as it does not any contain $\varepsilon$-arcs, and uses skip connections to represent consecutive deletions.

  \begin{prooftree}
    \AxiomC{$s\in\Sigma \phantom{\land} i \in [0, n] \phantom{\land} j \in [1, k]$}
    \RightLabel{$\duparrow$}
    \UnaryInfC{$(q_{i, j-1} \overset{s}{\rightarrow} q_{i,j}) \in \delta$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$s\in\Sigma \phantom{\land} i \in [1, n] \phantom{\land} j \in [1, k]$}
    \RightLabel{$\ddiagarrow$}
    \UnaryInfC{$(q_{i-1, j-1} \overset{s}{\rightarrow} q_{i,j}) \in \delta$}
  \end{prooftree}
  \begin{prooftree}
    \AxiomC{$i \in [1, n] \phantom{\land} j \in [0, k]$}
    \RightLabel{$\drightarrow$}
    \UnaryInfC{$(q_{i-1, j} \overset{\sigma_i}{\rightarrow} q_{i,j}) \in \delta$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$d \in [1, d_{\max}] \phantom{\land} i \in [d + 1, n] \phantom{\land} j \in [d, k]$}
    \RightLabel{$\knightarrow$}
    \UnaryInfC{$(q_{i-d-1, j-d} \overset{\sigma_i}{\rightarrow} q_{i,j}) \in \delta$}
  \end{prooftree}
  \begin{prooftree}
    \AxiomC{$\vphantom{|}$}
    \RightLabel{$\textsc{Init}$}
    \UnaryInfC{$q_{0,0} \in I$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$q_{i, j}$}
    \AxiomC{$|n-i+j| \leq k$}
    \RightLabel{$\textsc{Done}$}
    \BinaryInfC{$q_{i, j}\in F$}
  \end{prooftree}

  \newcommand{\substitutionExample}{
    \tikz{
      \foreach \x in {0,8,16,24,32,40}{
        \fill (\x pt,0pt) circle [radius = 1pt];
        \fill (\x pt,8pt) circle [radius = 1pt];
      }
      \phantom{\fill (0pt,-8pt) circle [radius = 1pt];}
      \draw [-to] (0pt,0pt) -- (8pt,0pt);
      \draw [-to] (8pt,0pt) -- (16pt,0pt);
      \draw [-to] (16pt,0pt) -- (24pt,8pt);
      \draw [-to] (24pt,8pt) -- (32pt,8pt);
      \draw [-to] (32pt,8pt) -- (40pt,8pt);
    }
  }

  \newcommand{\insertionExample}{
    \tikz{
      \foreach \x in {0,8,16,24,32,40}{
        \fill (\x pt,0pt) circle [radius = 1pt];
        \fill (\x pt,8pt) circle [radius = 1pt];
      }
      \phantom{\fill (0pt,-8pt) circle [radius = 1pt];}
      \fill[white] (16pt,0pt) circle [radius = 1.2pt];
      \fill[white] (24pt,8pt) circle [radius = 1.2pt];
      \draw [-to] (0pt,0pt) -- (8pt,0pt);
      \draw [-to] (8pt,0pt) -- (24pt,0pt);
      \draw [-to] (24pt,0pt) -- (16pt,8pt);
      \draw [-to] (16pt,8pt) -- (32pt,8pt);
      \draw [-to] (32pt,8pt) -- (40pt,8pt);
    }
  }

  \newcommand{\deletionExample}{
    \tikz{
      \foreach \x in {0,8,16,24,32,40}{
        \fill (\x pt,0pt) circle [radius = 1pt];
        \fill (\x pt,8pt) circle [radius = 1pt];
      }
      \phantom{\fill (0pt,-8pt) circle [radius = 1pt];}
      \draw [-to] (0pt,0pt) -- (8pt,0pt);
      \draw [-to] (8pt,0pt) -- (16pt,0pt);
      \draw [-to] (16pt,0pt) -- (24pt,0pt);
      \draw [-to] (24pt,0pt) -- (40pt,8pt);
    }
  }

  \newcommand{\doubleDeletionExample}{
    \tikz{
      \foreach \x in {0,8,16,24,32,40}{
        \fill (\x pt,0pt) circle [radius = 1pt];
        \fill (\x pt,8pt) circle [radius = 1pt];
        \fill (\x pt,16pt) circle [radius = 1pt];
      }
      \draw [-to] (0pt,0pt) -- (24pt,16pt);
      \draw [-to] (24pt,16pt) -- (32pt,16pt);
      \draw [-to] (32pt,16pt) -- (40pt,16pt);
    }
  }

  \newcommand{\subDelExample}{
    \tikz{
      \foreach \x in {0,8,16,24,32,40}{
        \fill (\x pt,0pt) circle [radius = 1pt];
        \fill (\x pt,8pt) circle [radius = 1pt];
        \fill (\x pt,16pt) circle [radius = 1pt];
      }
      \draw [-to] (0pt,0pt) -- (8pt,0pt);
      \draw [-to] (8pt,0pt) -- (16pt,8pt);
      \draw [-to] (16pt,8pt) -- (32pt,16pt);
      \draw [-to] (32pt,16pt) -- (40pt,16pt);
    }
  }

  \newcommand{\subSubExample}{
    \tikz{
      \foreach \x in {0,8,16,24,32,40}{
        \fill (\x pt,0pt) circle [radius = 1pt];
        \fill (\x pt,8pt) circle [radius = 1pt];
        \fill (\x pt,16pt) circle [radius = 1pt];
      }
      \draw [-to] (0pt,0pt) -- (8pt,0pt);
      \draw [-to] (8pt,0pt) -- (16pt,8pt);
      \draw [-to] (16pt,8pt) -- (24pt,16pt);
      \draw [-to] (24pt,16pt) -- (32pt,16pt);
      \draw [-to] (32pt,16pt) -- (40pt,16pt);
    }
  }

  \newcommand{\insertDeleteExample}{
    \tikz{
      \foreach \x in {0,8,16,24,32,40,48}{
        \fill (\x pt,0pt) circle [radius = 1pt];
        \fill (\x pt,8pt) circle [radius = 1pt];
        \fill (\x pt,16pt) circle [radius = 1pt];
      }
      \fill[white] (16pt,16pt) circle [radius = 1.2pt];
      \fill[white] (8pt,0pt) circle [radius = 1.2pt];
      \fill[white] (16pt,8pt) circle [radius = 1.2pt];
      \draw [-to] (0pt,0pt) -- (16pt,0pt);
      \draw [-to] (16pt,0pt) -- (8pt,8pt);
      \draw [-to] (8pt,8pt) -- (24pt,8pt);
      \draw [-to] (24pt,8pt) -- (40pt,16pt);
      \draw [-to] (40pt,16pt) -- (48pt,16pt);
    }
  }

  Each arc plays a specific role. $\duparrow$ handles insertions, $\ddiagarrow$ handles substitutions, $\duparrow$ handles insertions and $\knightarrow$ handles [consecutive] deletions of various lengths. Let us consider some illustrative cases.

  \begin{table}[h!]
    \begin{tabular}{ccccccc}

      \texttt{f\hspace{3pt}.\hspace{3pt}\hlorange{[}\hspace{3pt}x\hspace{3pt})} &
      \texttt{f\hspace{3pt}.\hspace{3pt}\phantom{(}\hspace{3pt}x\hspace{3pt})} &
      \texttt{f\hspace{3pt}.\hspace{3pt}(\hspace{3pt}\hlred{x}\hspace{3pt})} &
      \texttt{\hlred{.}\hspace{3pt}\hlred{+}\hspace{3pt}(\hspace{3pt}x\hspace{3pt})} &
      \texttt{f\hspace{3pt}\hlorange{.}\hspace{3pt}\hlred{(}\hspace{3pt}x\hspace{3pt};} &
      \texttt{[\hspace{3pt}\hlorange{,}\hspace{3pt}\hlorange{x}\hspace{3pt}y\hspace{3pt}]} &
      \texttt{[\hspace{3pt}\phantom{,}\hspace{3pt},\hspace{3pt}\hlred{x}\hspace{3pt}y\hspace{3pt}]} \\

      \texttt{f\hspace{3pt}.\hspace{3pt}\hlorange{(}\hspace{3pt}x\hspace{3pt})} &
      \texttt{f\hspace{3pt}.\hspace{3pt}\hlgreen{(}\hspace{3pt}x\hspace{3pt})} &
      \texttt{f\hspace{3pt}.\hspace{3pt}(\hspace{3pt}\phantom{x}\hspace{3pt})} &
      \texttt{\phantom{f}\hspace{3pt}\phantom{.}\hspace{3pt}(\hspace{3pt}x\hspace{3pt})} &
      \texttt{f\hspace{3pt}\hlorange{*}\hspace{3pt}\phantom{(}\hspace{3pt}x\hspace{3pt};} &
      \texttt{[\hspace{3pt}\hlorange{x}\hspace{3pt}\hlorange{,}\hspace{3pt}y\hspace{3pt}]} &
      \texttt{[\hspace{3pt}\hlgreen{x}\hspace{3pt},\hspace{3pt}\phantom{x}\hspace{3pt}y\hspace{3pt}]} \\

      \substitutionExample & \insertionExample & \deletionExample & \doubleDeletionExample & \subDelExample & \subSubExample & \insertDeleteExample
    \end{tabular}
  \end{table}

  Note that the same patch can have multiple Levenshtein alignments. $\textsc{Done}$ constructs the final states, which are all states accepting strings $\sigma'$ such that Levenshtein distance of $\Delta(\sigma, \sigma') \leq d_\max$.

  To avoid creating multiple arcs for each insertion or deletion, we alter the following rules:

  \begin{prooftree}
    \AxiomC{$\color{orange}S(s: \Sigma) \mapsto [s \neq \sigma_i] \color{black}\phantom{\land} i \in [0, n] \phantom{\land} j \in [1, k]$}
    \RightLabel{$\duparrow$}
    \UnaryInfC{$(q_{i, j-1} \overset{S}{\rightarrow} q_{i,j}) \in \delta$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$\color{orange}S(s: \Sigma) \mapsto [s \neq \sigma_i] \color{black}\phantom{\land} i \in [1, n] \phantom{\land} j \in [1, k]$}
    \RightLabel{$\ddiagarrow$}
    \UnaryInfC{$(q_{i-1, j-1} \overset{S}{\rightarrow} q_{i,j}) \in \delta$}
  \end{prooftree}

  By nominalizing the NFA, we can eliminate the creation of $e=|\Sigma|\cdot|\sigma|\cdot2d$ arcs for each terminal at each position in the string in the Levenshtein automaton, and $e^3$ productions in the construction to follow. Thus, it is absolutely essential to first nominalize the automaton before proceeding.

  \subsection{Levenshtein-Bar-Hillel Construction}

  We now describe the Bar-Hillel construction, which generates a grammar recognizing the intersection between a finite automaton, and then use the grammar to generate the edits without enumerating holes.

%  \begin{definition}
%    A finite state automaton is a tuple $\mathcal{A} = \langle Q, \Sigma, \delta, I, F\rangle$, where $Q$ is a finite set of states, $\Sigma$ is a finite alphabet, $\delta \subseteq Q \times \Sigma \times Q$ is the transition function, and $I, F \subseteq Q$ are the set of initial and final states, respectively.
%  \end{definition}

  \begin{lemma}\label{lemma:bar-hillel}
  For any context-free language $\ell$ and finite state automaton $\alpha$, there exists a context-free grammar $G_\cap$ such that $\mathcal{L}(G_\cap) = \ell \cap \mathcal{L}(\alpha)$. See Bar-Hillel~\cite{bar1961formal}.
  \end{lemma}

  \noindent Beigel and Gasarch~\cite{beigelproof} provide one explicit way to construct $G_\cap$:

  \begin{prooftree}
    \AxiomC{$q \in I \phantom{\land} r \in F\vphantom{\overset{a}{\rightarrow}}$}
%  \RightLabel{$\varepsilon^+\textsc{-int}$}
    \UnaryInfC{$\big(S\rightarrow q S r\big) \in P_\cap$}
    \DisplayProof
    \hskip 1em
    \AxiomC{$(q\overset{a}{\rightarrow}r) \in \delta$}
%  \RightLabel{$\varepsilon^+\textsc{-int}$}
    \UnaryInfC{$\big(qar\rightarrow a\big)\in P_\cap$}
    \DisplayProof
    \hskip 1em
%\end{prooftree}
%\begin{prooftree}
    \AxiomC{$(w \rightarrow xz) \in P\vphantom{\overset{a}{\rightarrow}}$}
    \AxiomC{$p,q,r \in Q$}
    \RightLabel{\textsc{\"w}}
    \BinaryInfC{$\big(pwr\rightarrow (pxq)(qzr)\big) \in P_\cap$}
  \end{prooftree}

  The standard BH construction applies to any CFL and REG language intersection, but this generates large grammar whose cardinality is approximately $|P_\cap|=|I||F| + |\delta| + |P||Q|^3$. Applying the BH construction directly to programming language syntax and Levenshtein automata can generate hundreds of trillions of productions for moderately sized grammars and Levenshtein automata. In the case of LEV-CFL intersection, this technique can be specialized to avoid creating many unnecessary productions. In this section, we describe a kind of reachability analysis that removes all nonterminals-state triples which is essential to making our approach tractable. We will now describe this reduction in detail.

%  To achieve this, we precompute upper and lower Parikh bounds for every terminal and integer range of the string, which we call the Parikh map. This construction soundly overapproximates the minimum and maximum number of terminals that can be derived from a given nonterminal in a bounded-length string, and is used to prune the search space. We will now describe this reduction in detail.

  Consider $\textsc{\"w}$. What this tells us is that each step in the intersection grammar is a set of paths sharing an origin and destination in the original automaton and a single step in the context-free grammar. A key thing to note here is that $\big(pwr\rightarrow (pxq)(qzr)\big)$ considers the interaction between every pair of states and every production, but this is a huge overapproximation. Most of these productions are completely useless. How do we know which ones to keep?

  Let us define a relation over the set of nonterminals in a grammar that soundly overapproximates the Parikh image. This will tell us the minimum and maximum number of symbols each nonterminal can represent.

  \begin{definition}[Parikh interval of a nonterminal]
    Let $p: \Sigma^*\rightarrow\mathbb{N}^{|\Sigma|}$ be the Parikh image~\cite{parikh1966context}, which counts the frequency of each terminal in a string. Let $\pi: V \rightarrow \{[a, b] \in \mathbb{N} \times \mathbb{N} \mid a \leq b\}^{|\Sigma|}$ be the smallest interval such that $\forall s: \Sigma^{[l, h]}, \forall v: V$, $v \Rightarrow^* s \vdash p(s) \in \pi(v)$.
  \end{definition}

  In other words, the Parikh interval computes the greatest lower and least upper bound of the Parikh image over all strings in the language of a nonterminal. The infimum of a nonterminal's Parikh interval tells us how many of each terminal a nonterminal must generate, and the supremum tells us how many it can generate. Likewise, we define a similar relation over NFA state pairs:

  \begin{definition}[Parikh interval of two NFA states]
    We define $\pi: Q\times Q \rightarrow \{[a, b] \in \mathbb{N} \times \mathbb{N} \mid a \leq b\}^{|\Sigma|}$ as the smallest interval such that $\forall \sigma: \Sigma^*, \forall q, q': Q$, $q \overset{s}{\Longrightarrow} q' \vdash p(\sigma) \in \pi(q, q')$.
  \end{definition}

  Now, we will define a distance between Parikh intervals, which will represent the minimum number of changes required to transform a string in one Parikh interval to a string in another.

  \begin{definition}[Parikh divergence]
    Given two Parikh intervals $\pi, \pi': \{[a, b] \in \mathbb{N} \times \mathbb{N} \mid a \leq b\}^{|\Sigma|}$, we define the divergence between them as $\pi \parallel \pi' \coloneqq \sum_{n=1}^{|\Sigma|} \min_{(i, i') \in \pi[n]\times \pi'[n]} |i - i'|$.
  \end{definition}

  Now we know that if the Parikh divergence is greater than the Levenshtein margin separating the two states, these two Parikh intervals must be incompatible.

  \begin{definition}[Levenshtein-Parikh compatibility]
    Let $\mu=q_{h,i}, \mu' = q_{j,k}$ be two states in a Lev-NFA and V be a CFG nonterminal. We say that $(\mu, \nu, \mu'): Q\times V\times Q$ are compatible iff the Parikh divergence is bounded by the Levenshtein margin $|k-i|$, i.e., $\nu \lhd \mu\mu' \iff (\pi(\nu) \parallel \pi(\mu, \mu')) \leq |k-i|$.
  \end{definition}

  We then intersect the Parikh intervals with all triples in the Levenshtein automaton to obtain the subset of $Q\times Q \times V$ to which the rule \textsc{\"w} applies. Concretely, we alter the \textsc{\"w} rule as follows:

\begin{prooftree}
  \AxiomC{$\color{orange} \exists s: \Sigma^*.q\overset{s}{\Longrightarrow} q' \phantom{\land} w \lhd pr  \phantom{\land} x \lhd pq \phantom{\land} z \lhd qr$}
  \AxiomC{$(w \rightarrow xz) \in P\vphantom{\overset{a}{\rightarrow}}$}
  \AxiomC{$p,q,r \in Q$}
  \RightLabel{\textsc{\"w}}
  \TrinaryInfC{$\big(pwr\rightarrow (pxq)(qzr)\big) \in P_\cap$}
\end{prooftree}

Specifically, we compute Parikh intervals generated by every path though the Levenshtein automaton, then intersect the Parikh intervals for the candidate nonterminals in question. Suppose we have a $p, q, r: Q$ and $w \rightarrow x z$, then check if $[\pi(q, q') \cap \pi(v) = \varnothing]$ for all $pwr, pxq, qzr$. If so, we can immediately rule out this tuple.

%To generate edits from it, we can use the same procedure as before, but instead of interleaving $\err\sigma$ with $\varepsilon$ and introducing holes, we simply use $A\big((\_)^{|\err{\sigma}| + d}\big, G_\cap)$.

  \subsection{Parsing as idempotent matrix completion}

  Recall that a CFG is a quadruple consisting of terminals $(\Sigma)$, nonterminals $(V)$, productions $(P\colon V \rightarrow (V \mid \Sigma)^*)$, and a start symbol, $(S)$. Every CFG is reducible to \textit{Chomsky Normal Form}, $P'\colon V \rightarrow (V^2 \mid \Sigma)$, in which every $P$ takes one of two forms, either $w \rightarrow xz$, or $w \rightarrow t$, where $w, x, z: V$ and $t: \Sigma$. For example:\vspace{-3pt}

  \begin{table}[H]
    \begin{tabular}{llll}
      $G\coloneqq\big\{\;S \rightarrow S\:S \mid (\:S\:) \mid (\:)\;\big\} \Longrightarrow \big\{\;S\rightarrow Q\:R \mid S\:S \mid L\:R,$ & $R \rightarrow\:),$ & $L \rightarrow (,$ & $Q\rightarrow L\:S\;\big\}$
    \end{tabular}
  \end{table}\vspace{-8pt}

  \noindent Given a CFG, $G' : \mathbb{G} = \langle \Sigma, V, P, S\rangle$ in CNF, we can construct a recognizer $R: \mathbb{G} \rightarrow \Sigma^n \rightarrow \mathbb{B}$ for strings $\sigma: \Sigma^n$ as follows. Let $2^V$ be our domain, $0$ be $\varnothing$, $\oplus$ be $\cup$, and $\otimes$ be defined as:\vspace{-10pt}

  \begin{align}
    X \otimes Z \coloneqq \big\{\;w \mid \langle x, z\rangle \in X \times Z, (w\rightarrow xz) \in P\;\big\}
  \end{align}

  \noindent If we define $\hat\sigma_r \coloneqq \{w \mid (w \rightarrow \sigma_r) \in P\}$, then construct a matrix with nonterminals on the superdiagonal representing each token, $M_0[r+1=c](G', \sigma) \coloneqq \;\hat\sigma_r$ and solve for the fixpoint $M_{i+1} = M_i + M_i^2$,\vspace{-10pt}

  \begin{align*}
    M_0\coloneqq
    \begin{pNiceMatrix}[nullify-dots,xdots/line-style=loosely dotted]
      \varnothing & \hat\sigma_1 & \varnothing & \Cdots & \varnothing \\
      \Vdots      & \Ddots   & \Ddots      & \Ddots & \Vdots\\
      &          &             &        & \varnothing\\
      &          &             &        & \hat\sigma_n \\
      \varnothing & \Cdots   &             &        & \varnothing
    \end{pNiceMatrix} &\Rightarrow
    \begin{pNiceMatrix}[nullify-dots,xdots/line-style=loosely dotted]
      \varnothing & \hat\sigma_1 & \Lambda & \Cdots & \varnothing \\
      \Vdots      & \Ddots   & \Ddots  & \Ddots & \Vdots\\
      &          &         &        & \Lambda\\
      &          &         &        & \hat\sigma_n \\
      \varnothing & \Cdots   &         &        & \varnothing
    \end{pNiceMatrix} &\Rightarrow \ldots \Rightarrow M_\infty =
    \begin{pNiceMatrix}[nullify-dots,xdots/line-style=loosely dotted]
      \varnothing & \hat\sigma_1 & \Lambda & \Cdots & \Lambda^*_\sigma\\
      \Vdots      & \Ddots   & \Ddots  & \Ddots & \Vdots\\
      &          &         &        & \Lambda\\
      &          &         &        & \hat\sigma_n \\
      \varnothing & \Cdots   &         &        & \varnothing
    \end{pNiceMatrix}
  \end{align*}

  \noindent we obtain the recognizer, $R(G', \sigma) \coloneqq [S \in \Lambda^*_\sigma] \Leftrightarrow [\sigma \in \mathcal{L}(G)]$~\footnote{Hereinafter, we use Iverson brackets to denote the indicator function of a predicate with free variables, i.e., $[P] \Leftrightarrow \mathds{1}(P)$.}.

  Since $\bigoplus_{c = 1}^n M_{r,c} \otimes M_{c,r}$ has cardinality bounded by $|V|$, it can be represented as $\mathbb{Z}_2^{|V|}$ using the characteristic function, $\mathds{1}$. A concrete example is shown in \S~\ref{sec:example}.

  Let us consider an example with two holes, $\sigma = 1$ \_ \_, and the grammar being $G\coloneqq\{S\rightarrow N O N, O \rightarrow + \mid \times, N \rightarrow 0 \mid 1\}$. This can be rewritten into CNF as $G'\coloneqq \{S \rightarrow N L, N \rightarrow 0 \mid 1, O \rightarrow × \mid +, L \rightarrow O N\}$. Using the algebra where $\oplus=\cup$, $X \otimes Z = \big\{\;w \mid \langle x, z\rangle \in X \times Z, (w\rightarrow xz) \in P\;\big\}$, the fixpoint $M' = M + M^2$ can be computed as follows, shown in the leftmost column:\\

  \begin{small}
  {\renewcommand{\arraystretch}{1.2}
  \noindent\phantom{...}\begin{tabular}{|c|c|c|c|}
                          \hline
                          & $2^V$ & $\mathbb{Z}_2^{|V|}$ & $\mathbb{Z}_2^{|V|}\rightarrow\mathbb{Z}_2^{|V|}$\\\hline
                          $M_0$ & \begin{pmatrix}
                                    \phantom{V} & \tiny{\{N\}} &         &             \\
                                    &              & \{N,O\} &             \\
                                    &              &         & \{N,O\} \\
                                    &              &         &
                          \end{pmatrix} & \begin{pmatrix}
                                            \phantom{V} & \ws\bs\ws\ws &              &              \\
                                            &              & \ws\bs\bs\ws &              \\
                                            &              &              & \ws\bs\bs\ws \\
                                            &              &              &
                          \end{pmatrix} & \begin{pmatrix}
                                            \phantom{V} & V_{0, 1} &          &          \\
                                            &          & V_{1, 2} &          \\
                                            &          &          & V_{2, 3} \\
                                            &          &          &
                          \end{pmatrix} \\\hline
                          $M_1$ & \begin{pmatrix}
                                    \phantom{V} & \tiny{\{N\}} & \varnothing &         \\
                                    &              & \{N,O\}     & \{L\}   \\
                                    &              &             & \{N,O\} \\
                                    &              &             &
                          \end{pmatrix} & \begin{pmatrix}
                                            \phantom{V} & \ws\bs\ws\ws & \ws\ws\ws\ws &              \\
                                            &              & \ws\bs\bs\ws & \bs\ws\ws\ws \\
                                            &              &              & \ws\bs\bs\ws \\
                                            &              &              &
                          \end{pmatrix} & \begin{pmatrix}
                                            \phantom{V} & V_{0, 1} & V_{0, 2} &          \\
                                            &          & V_{1, 2} & V_{1, 3} \\
                                            &          &          & V_{2, 3} \\
                                            &          &          &
                          \end{pmatrix} \\\hline
                          $M_\infty$ & \begin{pmatrix}
                                         \phantom{V} & \tiny{\{N\}} & \varnothing & \{S\}   \\
                                         &              & \{N,O\}     & \{L\}   \\
                                         &              &             & \{N,O\} \\
                                         &              &             &
                          \end{pmatrix} & \begin{pmatrix}
                                            \phantom{V} & \ws\bs\ws\ws & \ws\ws\ws\ws & \ws\ws\ws\bs \\
                                            &              & \ws\bs\bs\ws & \bs\ws\ws\ws \\
                                            &              &              & \ws\bs\bs\ws \\
                                            &              &              &
                          \end{pmatrix} & \begin{pmatrix}
                                            \phantom{V} & V_{0, 1} & V_{0, 2} & V_{0, 3} \\
                                            &          & V_{1, 2} & V_{1, 3} \\
                                            &          &          & V_{2, 3} \\
                                            &          &          &
                          \end{pmatrix}\\\hline
  \end{tabular}\\
  }
  \end{small}

  The same procedure can be translated, without loss of generality, into the bit domain ($\mathbb{Z}_2^{|V|}$) using a lexicographic ordering, however these both are recognizers. That is to say, $[S\in V_{0, 3}]\Leftrightarrow [V_{0, 3, 3}=\bs] \Leftrightarrow [A(\sigma) \neq \varnothing]$. Since $V_{0, 3} = \{S\}$, we know there is at least one $\sigma' \in A(\sigma)$, but $M_\infty$ does not reveal its identity.

%$\{\text{xor}, \land, \top\}$ is a functionally complete set is equivalent to $\mathbb{Z}_2$ $\top := 1, \land := \times, \text{xor} := +$. We can define $=$ as $(a = b) \Leftrightarrow (a \text{ xor } b) \text{ xor } \top \Leftrightarrow (a + b) + \top$.

  In order to extract the inhabitants, we can translate the bitwise procedure into an equation with free variables. Here, we can encode the idempotency constraint directly as $M = M^2$. We first define $X \boxtimes Z = [X_2 \land Z_1, \bot, \bot, X_1 \land Z_0]$ and $X \boxplus Z = [X_i \lor Z_i]_{i \in [0, |V|)}$. Since the unit nonterminals $O, N$ can only occur on the superdiagonal, they may be safely ignored by $\otimes$. To solve for $M_\infty$, we proceed by first computing $V_{0, 2}, V_{1, 3}$ as follows:

  \begin{align}
    V_{0, 2} &= V_{0, j} \cdot V_{j, 2} = V_{0, 1} \boxtimes V_{1, 2}\\
    &= [L \in V_{0, 2}, \bot, \bot, S \in V_{0, 2}]\\
    &= [O \in V_{0, 1} \land N \in V_{1, 2}, \bot, \bot, N \in V_{0, 1} \land L \in V_{1, 2}]\\
    &= [V_{0, 1, 2} \land V_{1, 2, 1}, \bot, \bot, V_{0, 1, 1} \land V_{1, 2, 0}]
  \end{align}

  \begin{align}
    V_{1, 3} &= V_{1, j} \cdot V_{j, 3} = V_{1, 2} \boxtimes V_{2, 3}\\
    &= [L \in V_{1, 3}, \bot, \bot, S \in V_{1, 3}]\\
    &= [O \in V_{1, 2} \land N \in V_{2, 3}, \bot, \bot, N \in V_{1, 2} \land L \in V_{2, 3}]\\
    &= [V_{1, 2, 2} \land V_{2, 3, 1}, \bot, \bot, V_{1, 2, 1} \land V_{2, 3, 0}]
  \end{align}

  Now we solve for the corner entry $V_{0, 3}$ by taking the bitwise dot product between the first row and last column, yielding:

  \begin{align}
    V_{0, 3} &= V_{0, j} \cdot V_{j, 3} = V_{0, 1} \boxtimes V_{1, 3} \boxplus V_{0, 2} \boxtimes V_{2, 3}\\
%  &= [V_{0, 1, 2} \land V_{1, 3, 1}, \bot, \bot, V_{0, 1, 1} \land V_{1, 3, 0}] + [V_{0, 2, 2} \land V_{2, 3, 1}, \bot, \bot, V_{0, 2, 1} \land V_{2, 3, 0}]\\
    &= [V_{0, 1, 2} \land V_{1, 3, 1} \lor V_{0, 2, 2} \land V_{2, 3, 1}, \bot, \bot, V_{0, 1, 1} \land V_{1, 3, 0} \lor V_{0, 2, 1} \land V_{2, 3, 0}]
  \end{align}

  Since we only care about $V_{0, 3, 3} \Leftrightarrow [S \in V_{0, 3}]$, so we can ignore the first three entries and solve for:

  \begin{align}
    V_{0, 3, 3} &= V_{0, 1, 1} \land V_{1, 3, 0} \lor V_{0, 2, 1} \land V_{2, 3, 0}\\
    &= V_{0, 1, 1} \land (V_{1, 2, 2} \land V_{2, 3, 1}) \lor V_{0, 2, 1} \land \bot\\
    &= V_{0, 1, 1} \land V_{1, 2, 2} \land V_{2, 3, 1}\\
    &= [N \in V_{0, 1}] \land [O \in V_{1, 2}] \land [N \in V_{2, 3}]
  \end{align}

  Now we know that $\sigma =$ 1 \underline{O} \underline{N} is a valid solution, and therefor we can take the product $\{1\}\times \hat\sigma_r^{-1}(O) \times \hat\sigma_r^{-1}(N)$ to recover the admissible set, yielding $A(\sigma)=\{1+0, 1+1, 1\times 0, 1\times 1\}$. In this case, since $G$ is unambiguous, there is only one parse tree satisfying $V_{0, |\sigma|, |\sigma|}$, but in general, there can be multiple valid parse trees, in which case we can decode them incrementally.

  The question naturally arises, where should one put the holes? One solution is to interleave $\varepsilon$ between every token as $\err{\sigma}_\varepsilon\coloneqq \left(\varepsilon^c\err{\sigma}_i\right)_{i=1}^n\varepsilon^c$, augment the grammar to admit $\varepsilon^+$, then sample holes without replacement from all possible locations. Below we illustrate this procedure on a single Python snippet.

%A well-known result in FL theory is that the class of context-free languages are closed under intersection with regular languages, i.e.,
%
%\begin{align}
%  \ell_1:\textsc{Reg}, \ell_2: \textsc{Cfl} \vdash \text{ there exists } G \text{ s.t. } L(G): \textsc{Cfl} \text{ and } L(G) = \ell_1\cap\ell_2
%\end{align}
%
%To compute the intersection between a CFG and a regular grammar, there is a standard construction from Beigel and Gasarch~\cite{beigelproof}, which allows us to compute the intersection grammar. We can then use the same procedure as before to compute the fixpoint, $M_\infty$, and extract the inhabitants.
%
%Alternatively, the procedure we use is to generate the contents of the grammar incrementally by sampling holes without replacement. We illustrate the procedure below, then discuss the theory.

  \begin{enumerate}[leftmargin=.23\linewidth]
    \item \texttt{d = sum([foo(i\err{]} for i in vals))}
    \item \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
            \hline
            \texttt{d} & \texttt{=} & \texttt{sum} & \texttt{(} & \texttt{[} & \texttt{foo} & \texttt{(} & \texttt{i} & \texttt{]} & \texttt{for} & \texttt{i} & \texttt{in} & \texttt{vals} & \texttt{)} & \texttt{)} \\\hline
    \end{tabular}
    \item \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
            \hline
            \texttt{w} & \texttt{=} & \texttt{w} & \texttt{(} & \texttt{[} & \texttt{w} & \texttt{(} & \texttt{w} & \texttt{]} & \texttt{for} & \texttt{w} & \texttt{in} & \texttt{w} & \texttt{)} & \texttt{)} \\\hline
    \end{tabular}
    \item \begin{tabular}{|||c|||c|||c|||c|||c|||c|||c|||c|||c|||c|||c|||c|||c|||c|||c|||}
            \hline
            \texttt{w} & \texttt{=} & \texttt{w} & \texttt{(} & \texttt{[} & \texttt{w} & \texttt{(} & \texttt{w} & \texttt{]} & \texttt{for} & \texttt{w} & \texttt{in} & \texttt{w} & \texttt{)} & \texttt{)} \\\hline
    \end{tabular}
    \item \begin{tabular}{|||c|||c|||c|||c|||c|||c|||c|||c|||c|||c|||c|||c|||c|||c|||c|||}
            \hline
            \cellcolor{black!15}\texttt{\_} & \texttt{=} & \cellcolor{black!15}\texttt{\_} & \texttt{(} & \texttt{[} & \texttt{w} & \texttt{(} & \texttt{w} & \texttt{]} & \texttt{for} & \texttt{w} & \texttt{in} & \texttt{w} & \texttt{)} & \cellcolor{black!15}\texttt{\_} \\\hline
    \end{tabular}\\
    \begin{tabular}{|||c|||c|||c|||c|||c|||c|||c|||c|||c|||c|||c|||c|||c|||c|||c|||}
      \hline
      \texttt{w} & \cellcolor{black!15}\texttt{\_} & \texttt{w} & \texttt{(} & \texttt{[} & \cellcolor{black!15}\texttt{\_} & \texttt{(} & \texttt{w} & \texttt{]} & \texttt{for} & \texttt{w} & \cellcolor{black!15}\texttt{\_} & \texttt{w} & \texttt{)} & \texttt{)} \\\hline
%          \cellcolor{black!15}\texttt{\_} & \texttt{=} & \cellcolor{black!15}\texttt{\_} & \texttt{(} & \texttt{[} & \texttt{w} & \texttt{(} & \texttt{w} & \texttt{]} & \texttt{for} & \texttt{w} & \texttt{in} & \texttt{w} & \texttt{)} & \cellcolor{black!15}\texttt{\_} \\\hline
%          & & & & & & & & & & & & & & \\\hline
    \end{tabular}\\$\cdots$
    \item \begin{tabular}{|||c|||c|||c|||c|||c|||c|||c||c|c|||c|||c|||c|||c|||c|||c|||c|||}
            \hline
%          \texttt{w} & \texttt{=} & & \texttt{w} & \texttt{(} & \texttt{[} & \texttt{w} & \texttt{(} & \texttt{w} & \texttt{]} & \texttt{for} & \texttt{w} & \texttt{in} & \texttt{w} & \texttt{)} & \texttt{)} \\\hline
            \texttt{w} & \texttt{=} & \texttt{w} & \texttt{(} & \texttt{[} & \texttt{w} & \texttt{(} & \cellcolor{black!15}\texttt{\_} &  \texttt{w} & \cellcolor{black!15}\texttt{\_} & \texttt{for} & \texttt{w} & \texttt{in} & \texttt{w} & \cellcolor{black!15}\texttt{\_} & \texttt{)} \\\hline
            & & & & & & & \cellcolor{green!25}\texttt{+} & & \cellcolor{orange!25}\texttt{)} & & & & & \cellcolor{orange!25}\texttt{]} & \\\hline
    \end{tabular}
    \item \texttt{d = sum([foo(\hlgreen{+}i\hlorange{)} for i in vals\hlorange{]})}
    \item \texttt{d = sum([foo(i\hlorange{)} for i in vals\hlorange{]})}
  \end{enumerate}

  The initial broken string, \texttt{d = sum([foo(i\err{]} for i in vals))} (1), is first tokenized using a lexer to obtain the sequence in (2).

  \subsection{A Pairing Function for Breadth-Bounded Binary Trees}\label{sec:pairing}

  Now that we have a method for parsing with matrix completion, we will translate this to the tree domain, where the semiring algebra will be defined over indexed forests. Then we will show how to sample trees.

  \begin{wrapfigure}{r}{0.45\textwidth}
    \resizebox{0.45\textwidth}{!}{
      \begin{tikzpicture}
      [
        grow                    = right,
        sibling distance        = 3em,
        level distance          = 5em,
        edge from parent/.style = {draw, -latex},
        every node/.style       = {font=\footnotesize},
        sloped,
        treenode/.style = {shape=rectangle, rounded corners,
        draw, align=center,
        top color=white, bottom color=blue!20},
        root/.style     = {treenode, font=\tiny, bottom color=red!30},
        env/.style      = {treenode, font=\tiny},
        dummy/.style    = {circle,draw}
      ]
        \node [root] {S}
        child { node [env] {BC}
        child { node [root] {B}
        child { node [env] {RD}
        child { node [root] {R} edge from parent node [above] }
        child { node [root] {D} edge from parent node [above] }
        edge from parent node [above] }
        edge from parent node [below] }
        child { node [root] {C}
        child { node [env] {\ldots\vphantom{BB}} edge from parent node [below] }
%  child { edge from parent node [above] {\ldots} }
        edge from parent node [below] }
        edge from parent node [above] }
        child { node [env] {\ldots\vphantom{BB}} edge from parent node [below] }
        child { node [env] {AB}
        child { node [root] {A}
        child {
          node [env] {QC}
          child { node [root] {Q} edge from parent node [above] }
          child { node [root] {C} edge from parent node [above] }
          edge from parent node [above]
        }
%    child { node [env] {ZQ} edge from parent node [above] }
        child { node [env] {\ldots\vphantom{BB}} edge from parent node [below] }
        edge from parent node [below] }
        child { node [root] {B}
        child { node [env] {RD}
        child { node [root] {R} edge from parent node [above] }
        child { node [root] {D} edge from parent node [above] }
        edge from parent node [above] }
        edge from parent node [below] }
        edge from parent node [above] };
      \end{tikzpicture}
    }
    \caption{A partial $\mathbb{T}_2$ for the grammar $P=\{S \rightarrow BC \mid \ldots \mid AB, B\rightarrow RD \mid \ldots, A\rightarrow QC \mid \ldots\}$.}
  \end{wrapfigure}

  We will now describe a technique for sampling trees from the intersection grammar, representing distinct, fully formed repairs. When the number of choices is sufficiently constrained, this can be sampled without replacement, or otherwise with replacement using a PCFG. Once we have obtained the intersection grammar, we solve for all inhabitants using a matrix fixedpoint recurrence.

  We define an algebraic data type $\mathbb{T}_3 = (V \cup \Sigma) \rightharpoonup \mathbb{T}_2$ where $\mathbb{T}_2 = (V \cup \Sigma) \times (\mathbb{N} \rightharpoonup \mathbb{T}_2\times\mathbb{T}_2)$\footnote{Given a $T:\mathbb{T}_2$, we may also refer to $\pi_1(T), \pi_2(T)$ as $\texttt{root}(T)$ and $\texttt{children}(T)$ respectively, where children are pairs of conjoined twins.}. Morally, we can think of $\mathbb{T}_2$ as an implicit set of possible trees sharing the same root, and $\mathbb{T}_3$ as a dictionary of possible $\mathbb{T}_2$ values indexed by possible roots, given by a specific CFG under a finite-length porous string. We construct $\hat\sigma_r = \Lambda(\sigma_r)$ as follows:

\vspace{-10pt}\begin{equation*}
  \begin{footnotesize}
\Lambda(s: \underline\Sigma) \mapsto \begin{cases}
\bigoplus_{s\in \Sigma} \Lambda(s) & \text{if $s$ is a hole,} \vspace{5pt}\\
\big\{\mathbb{T}_2\big(w, \big[\langle\mathbb{T}_2(s), \mathbb{T}_2(\varepsilon)\rangle\big]\big) \mid (w \rightarrow s)\in P\big\} & \text{otherwise.}
\end{cases}
  \end{footnotesize}
\end{equation*}

\noindent This initializes the superdiagonal entries, enabling us to compute the fixpoint $M_\infty$ by redefining $\oplus, \otimes: \mathbb{T}_3 \times \mathbb{T}_3 \rightarrow \mathbb{T}_3$ as:

\begin{equation*}
  \begin{footnotesize}
  X \oplus Z \mapsto \bigcup_{\mathclap{k\in \pi_1(X \cup Z)}}\Big\{k \Rightarrow \mathbb{T}_2(k, x \cup z) \mid x \in \pi_2(X\circ k), z \in \pi_2(Z\circ k)\Big\}
  \end{footnotesize}
\end{equation*}

\begin{equation*}
  \begin{footnotesize}
  X \otimes Z \mapsto \bigoplus_{\mathclap{(w\rightarrow xz) \in P}}\Big\{\mathbb{T}_2\Big(w, \big[\langle X\circ x, Z\circ z\rangle\big]\Big) \mid x \in \pi_1(X), z \in \pi_1(Z)\Big\}
\end{footnotesize}
\end{equation*}

These operators group subtrees by their root nonterminal, then aggregate their children. Instead of tracking sets, each $\Lambda$ now becomes a dictionary indexed by the root nonterminal, which can be sampled by obtaining $(\Lambda_\sigma^* \circ S): \mathbb{T}_2$, then recursively choosing twins as we describe in \S~\ref{sec:replacement}, or without replacement via enumeration as described in \S~\ref{sec:pairing}.

%\begin{equation*}
%  \mathcal{C}(T: \mathbb{T}_2) \mapsto \begin{cases}
%    \texttt{root}(T) & \text{if $T$ is a leaf,} \\
%    \big\{x z \mid \langle X, Z\rangle \in \texttt{children}(T), x \in \mathcal{C}(X), z \in \mathcal{C}(Z)\big\} & \text{otherwise.}%\text{if $d \leq \max(|\err{\sigma}|, \min_{\sigma \in \mathcal{L}(G')}|\sigma|)$}.
%  \end{cases}
%\end{equation*}

Given a probabilistic CFG whose productions indexed by each nonterminal are decorated with a probability vector $\mathbf{p}$ (this may be uniform in the non-probabilistic case), we define a tree sampler $\Gamma: (\mathbb{T}_2 \mid \mathbb{T}_2^2) \rightsquigarrow \mathbb{T}$ which recursively samples children according to a Multinoulli distribution:

\begin{equation*}
  \Gamma(T) \mapsto \begin{cases}
        \Gamma\big(\text{Multi} \big(\texttt{children}(T), \mathbf{p}\big)\big) & \text{ if $T: \mathbb{T}_2$ } \\
        \big\langle \Gamma\big(\pi_1(T)\big), \Gamma\big(\pi_2(T)\big) \big\rangle & \text{ if $T: \mathbb{T}_2\times\mathbb{T}_2$ }
  \end{cases}
\end{equation*}

This is closely related to the generating function for the ordinary Boltzmann sampler from analytic combinatorics,

\begin{equation*}
  \Gamma C(x) \mapsto \begin{cases}
  \text{Bern} \left(\frac{A(x)}{A(x) + B(x)}\right) \rightarrow \Gamma A(x) \mid \Gamma B(x) & \text{ if } \mathcal{C}=\mathcal{A}+\mathcal{B} \\
  \big\langle \Gamma A(x), \Gamma B(x)\big\rangle & \text{ if } \mathcal{C}=\mathcal{A} \times \mathcal{B}
  \end{cases}
\end{equation*}

\noindent however unlike Duchon et al.~\cite{duchon2004boltzmann}, our work does not depend on rejection to guarantee exact-size sampling, as all trees contained in $\mathbb{T}_2$ will necessarily be the same width.

The type $\mathbb{T}_2$ of all possible trees that can be generated by a CFG in Chomksy Normal Form corresponds to the fixpoints of the following recurrence, which tells us that each $\mathbb{T}_2$ can be a terminal, nonterminal, or a nonterminal and a sequence consisting of nonterminal pairs and their two children:\vspace{-10pt}

\begin{equation*}
  L(p) = 1 + p L(p) \phantom{addspace} P(a) = \Sigma + V + V L\big(V^2P(a)^2\big)
\end{equation*}

Given a $\sigma: \underline\Sigma$, we construct $\mathbb{T}_2$ from the bottom-up, and sample from the top-down. Depicted below is a partial $\mathbb{T}_2$, where red nodes are \texttt{root}s and blue nodes are \texttt{children}:


The number of binary trees inhabiting a single instance of $\mathbb{T}_2$ is sensititive to the number of nonterminals and rule expansions in the grammar. To obtain the total number of trees with breadth $n$, we abstractly parse the porous string using the algebra defined in \S~\ref{sec:background}, letting $T=\Lambda_{\underline\sigma}^* \circ S$, and compute the total number of trees using the recurrence:

\begin{equation*}
  |T: \mathbb{T}_2| \mapsto \begin{cases}
%    \big|\{s \mid \big(\texttt{root}(T) \rightarrow s\big) \in P^\cap\}\big| & \text{if $T$ is a leaf,} \\
    1  & \text{if $T$ is a leaf,} \\
    \sum_{\langle T_1, T_2\rangle \in \texttt{children}(T)} |T_1| \cdot |T_2| & \text{otherwise.}
  \end{cases}
\end{equation*}

To sample all trees in a given $T: \mathbb{T}_2$ uniformly without replacement, we then construct a modular pairing function $\varphi: \mathbb{T}_2 \rightarrow \mathbb{Z}_{|T|} \rightarrow \texttt{BTree}$, which is defined as follows:

\begin{small}
\begin{equation*}
  \varphi(T: \mathbb{T}_2, i: \mathbb{Z}_{|T|}) \mapsto \begin{cases}
  \Big\langle\texttt{BTree}\big(\texttt{root}(T)\big), i\Big\rangle & \text{if $T$ is a leaf,} \vspace{5pt}\\
  \text{Let } b = |\texttt{children}(T)|,\\
  \phantom{\text{Let }} q_1, r=\big\langle\lfloor\frac{i}{b}\rfloor, i \pmod{b}\big\rangle,\\
  \phantom{\text{Let }} lb, rb = \texttt{children}[r],\\
  \phantom{\text{Let }} T_1, q_2 = \varphi(lb, q_1),\\
  \phantom{\text{Let }} T_2, q_3 = \varphi(rb, q_2) \text{ in } \\
  \Big\langle\texttt{BTree}\big(\texttt{root}(T), T_1, T_2\big), q_3\Big\rangle & \text{otherwise.} \\
  \end{cases}
\end{equation*}
\end{small}

Then, instead of sampling trees, we can simply sample integers uniformly without replacement from $\mathbb{Z}_{|T|}$ using the construction defined in \S~\ref{sec:method}, and lazily decode them into trees.

  \section{Dataset}

  The StackOverflow dataset is comprised of 500k Python code snippets, each of which has been annotated with a human repair. We depict the normalized edit loations relative to the snippet length below.

  \begin{figure}
    \begin{tikzpicture}
      \begin{axis}[
      ybar,
      bar width=15pt,
      xlabel={Beginning of snippet $\longleftrightarrow$ End of snippet},
      ylabel={Frequency},
      title={Normalized edit locations},
      ymin=0,
      ymax=35,
      xtick=data,
      xticklabels={10\%,20\%,30\%,40\%,50\%,60\%,70\%,80\%,90\%,100\%},
      ymajorgrids=true,
      grid style=dashed,
      width=\textwidth,
      height=0.3\textwidth
      ]

      \addplot table {
        X Y
        10 11.6539
        20 5.7252
        30 6.2087
        40 5.9542
        50 5.5980
        60 7.9389
        70 7.0738
        80 6.9466
        90 12.4173
        100 30.4835
      };
      \end{axis}
    \end{tikzpicture}
  \end{figure}

  \noindent Likewise, we can plot the number of tokens between edits within each patch:

  \begin{figure}
    \begin{tikzpicture}
      \begin{axis}[
        ybar,
        bar width=15pt,
        title={Intra-patch edit distance},
        xlabel={Caret distance},
        ylabel={Frequency},
        xtick=data,
        ymajorgrids=true,
        grid style=dashed,
        xticklabels={1,2,3,4,5,6,7,8,9,10+},
        width=\textwidth,
        height=0.3\textwidth
      ]

        \addplot table {
          X Y
          1 40.66
          2 15.00
          3 5.80
          4 4.86
          5 4.26
          6 2.98
          7 2.05
          8 2.73
          9 1.62
          10 13.64
        };
      \end{axis}
    \end{tikzpicture}
  \end{figure}

  \input{eval_contents}

  \subsection{Old Evaluation}

  We evaluate Tidyparse along three primary axes: latency, throughput, and accuracy on a dataset of human repairs. Our intention here is to show that Tidyparse is competitive with a large language model (roughly, a deep circuit) that is slow but highly sample-efficient with a small language model (roughly, a shallow circuit) that is fast but less sample-efficient.

  Large language models typically take between several hundred milliseconds and several seconds to infer a repair. The output is not guaranteed to be syntactically valid, and may require more than one sample to discover a valid repair. In contrast, Tidyparse can discover thousands of repairs in the same duration, all of which are guaranteed to be syntactically valid. Furthermore, if a valid repair exists within a certain number of edits, it will eventually be found.

  To substantiate these claims, we conduct experiments measuring:

  \begin{itemize}
    \item the average worst-case time to discover a human repair across varying sizes, i.e., average latency to discover a repair with edit distance $d$.
    \item the average accuracy at varying latency cutoffs, i.e., average precision@k at latency cutoff $t$.
    \item the average repair throughput across varying CPU cores, i.e., average number of admissible repairs discovered per second over the repair length.
    \item the relative throughput versus a uniform sampler, i.e., average number of admissible repairs discovered per second divided by the uniform sampler's throughput
  \end{itemize}

  \subsection{Uniform sampling benchmark}\label{sec:uniform}

  Below, we plot the precision of the uniform sampling procedure described in \S\ref{sec:dsi} against human repairs of varying edit distances and latency cutoffs. Repairs discovered before timeout expiration are reranked by tokenwise perplexity then compared using an exact lexical match with the human repair at or below rank k. We note that the uniform sampling procedure is not intended to be used in practice, but rather provides a baseline for the empirical density of the admissible set, and an upper bound on the latency required to attain a given precision.

  \begin{figure}[H]
    \resizebox{.3\textwidth}{!}{\input{repair1-3_plot.tex}}
    \resizebox{.3\textwidth}{!}{\input{repair1_plot.tex}}
    \resizebox{.3\textwidth}{!}{\input{repair2_plot.tex}}
%\resizebox{.3\textwidth}{!}{\input{repair1_plot.tex}}
%\resizebox{.307\textwidth}{!}{\input{repair2_plot.tex}}
    \caption{Human repair benchmark. Note the y-axis across different edit distance plots has varying ranges.}\label{fig:human}
  \end{figure}

  Despite the high-latency, this demonstrates a uniform prior with post-timeout reranking is still able to achieve competitive precision@k using a relatively cheap ranking metric. This suggests that we can use the metric to bias the sampler towards more likely repairs, which we will now do.

%\begin{figure}[h]
%\resizebox{0.3\textwidth}{!}{
%\begin{tikzpicture}
%\begin{axis}[
%ybar,
%enlarge x limits=0.15,
%legend style={fill opacity=0.8, draw opacity=1, text opacity=1, draw=lightgray204, legend pos=outer north east},
%ylabel={Precision@1},
%xtick={1,2,3},
%xlabel={Edit Distance},
%xtick=data,
%nodes near coords,
%nodes near coords align={vertical},
%ymin=0,ymax=1,
%]
%\addplot coordinates {(1, 0.9) (2, 0.857) (3, 0.794)};
%\addplot coordinates {(1, 0.454) (2, 0.162) (3, 0.096)};
%\addplot coordinates {(1, 0.005) (2, 0.004) (3, 0.0)};
%\legend{Syntactic, AbstractEval, CharMatch}
%\end{axis}
%\end{tikzpicture}
%}
%\resizebox{0.3\textwidth}{!}{
%\begin{tikzpicture}
%\begin{axis}[
%ybar,
%enlarge x limits=0.15,
%legend style={fill opacity=0.8, draw opacity=1, text opacity=1, draw=lightgray204, legend pos=outer north east},
%ylabel={Latency (ms)},
%xtick={1,2,3},
%xlabel={Edit Distance},
%xtick=data,
%nodes near coords,
%nodes near coords align={vertical},
%ymin=0,ymax=3000,
%]
%\phantom{\legend{Syntactic, AbstractEval, CharMatch}}
%\addplot coordinates {(1, 1585.465) (2, 1953.56) (3, 2744.887)};
%\end{axis}
%\end{tikzpicture}
%}
%\caption{Seq2Parse precision@1 and latency on the StackOverflow dataset.}
%\end{figure}

  \subsection{Repair with an adaptive sampler}

  In the following benchmark, we measure the precision@k of our repair procedure against human repairs of varying edit distances and latency cutoffs, using an adaptive resampling procedure described in \S\ref{sec:adaptive}. This sampler maintains a buffer of successful repairs ranked by perplexity and uses stochastic local search to resample edits within a neighborhood. Initially, edits are sampled uniformly at random. Over time and as the admissible set grows, it prioritizes edits nearby low-perplexity repairs. This technique offers a significant advantage in the low-latency setting.

  \begin{figure}[H]
    \resizebox{.24\textwidth}{!}{\input{repair1-3_10s_plot.tex}}
    \resizebox{.25\textwidth}{!}{\input{repair1_10s_plot.tex}}
    \resizebox{.24\textwidth}{!}{\input{repair2_10s_plot.tex}}
    \resizebox{.24\textwidth}{!}{\input{repair3_10s_plot.tex}}
    \caption{Adaptive sampling repairs. The red line indicates Seq2Parse precision@1 on the same dataset. Since it only supports generating one repair, we do not report precision@k or the intermediate latency cutoffs.}\label{fig:adaptive}
  \end{figure}

  We also evaluate Seq2Parse on the same dataset. Seq2Parse only supports precision@1 repairs, and so we only report Seq2parse precision@1 from the StackOverflow benchmark for comparison. Unlike our approach which only produces syntactically correct repairs, Seq2Parse also produces syntactically incorrect repairs and so we report the percentage of repairs matching the human repair for both our method and Seq2Parse. Seq2Parse latency varies depending on the length of the repair, averaging 1.5s for $\Delta=1$ to 2.7s for $\Delta=3$, across the entire StackOverflow dataset.

  While adapting sampling is able to saturate the admissible set for 1- and 2-edit repairs before the timeout elapses, 3-edit throughput is heavily constrained by compute around 16 lexical tokens, when Python's Levenshtein ball has a volume of roughly $6\times 10^8$ edits. This bottleneck can be relaxed with a longer timeout or additional CPU cores. Despite the high computational cost of sampling multi-edit repairs, our precision@all remains competitive with the Seq2Parse neurosymbolic baseline at the same latency. We provide some qualitative examples of repairs in Table~\ref{sec:appendix}.

  \subsection{Throughput benchmark}

  \begin{wrapfigure}{r}{0.3\textwidth}
    \resizebox{.3\textwidth}{!}{\input{throughput.tex}}
    \label{fig:throughput}
    \vspace{-30pt}
  \end{wrapfigure}

  End-to-end throughput varies significantly with the edit distance of the repair. Some errors are trivial to fix, while others require a large number of edits to be sampled before a syntactically valid edit is discovered. We evaluate throughput by sampling edits across invalid strings $|\err\sigma| \leq 40$ from the StackOverflow dataset of varying length, and measure the total number of syntactically valid edits discovered, as a function of string length and language edit distance $\Delta\in[1, 3]$. Each trial is terminated after 10 seconds, and the experiment is repeated across 7.3k total repairs. Note the y-axis is log-scaled, as the number of admissible repairs increases sharply with language edit distance. Our approach discovers a large number of syntactically valid repairs in a relatively short amount of time, and is able to quickly saturate the admissible set for 1- and 2-edit repairs before timeout. As the Seq2Parse baseline is unable to generate more than one syntactically valid repair per string, we do not report its throughput.

  \subsection{Synthetic repair benchmark}\label{sec:latency}

  In addition to the StackOverflow dataset, we also evaluate our approach on two datasets containing synthetic strings generated by a Dyck language, and bracketing errors of synthetic and organic provenance in organic source code. The first dataset contains length-50 strings sampled from various Dyck languages, i.e., the Dyck language containing n different types of balanced parentheses. The second contains abstracted Java and Python source code mined from GitHub repositories. The Dyck languages used in the remaining experiments are defined by the following context-free grammar(s):

  \begin{wholetidyinput}
    Dyck-1 -> ( ) | ( Dyck-1 ) | Dyck-1 Dyck-1
    Dyck-2 -> Dyck-1 | [ ] | ( Dyck-2 ) | [ Dyck-2 ] | Dyck-2 Dyck-2
    Dyck-3 -> Dyck-2 | { } | ( Dyck-3 ) | [ Dyck-3 ] | { Dyck-3 } | Dyck-3 Dyck-3
  \end{wholetidyinput}

  \noindent In experiment (1a), we sample a random valid string $\sigma \sim \Sigma^{50} \cap \mathcal{L}_{\text{Dyck-n}}$, then replace a fixed number of indices in $[0, |\sigma|)$ with holes and measure the average time required to decode ten syntactically-admissible repairs across 100 trial runs. In experiment (1b), we sample a random valid string as before, but delete p tokens at random and rather than provide their location(s), ask our model to solve for both the location(s) and repair by sampling uniformly from all n-token HCs, then measure the total time required to decode the first admissible repair. Note the logarithmic scale on the y-axis.

  \begin{figure}[H]
    \begin{minipage}{.48\textwidth}
      \begin{center}\footnotesize\textbf{Synthetic bracket language}\end{center}
    \end{minipage}
    \begin{minipage}{.48\textwidth}
      \begin{center}\footnotesize\textbf{Organic bracket language}\end{center}
    \end{minipage}\\
    \vspace{10pt}
    \hspace{-0.25cm}\begin{tikzpicture}[scale=0.35]
                      \begin{axis}[
                        width=8.3cm,
                        height=7cm,
                        title={\hspace{-1cm}\textbf{(1a) Latency with known locations}},
                        ybar,
                        bar width=6pt,
                        xlabel={Number of holes},
                        ylabel={ms to synthesize 10 repairs},
                        xtick=data,
                        axis x line*=bottom,
                        axis y line*=left,
                        ytick pos=left,
                        xticklabels from table={\loctimings}{holes},
                        ymajorgrids,
                        legend pos=north west,
                        legend columns=2,
                        error bars/y dir=both,
                        error bars/y explicit
                      ]
                        \addplot table [x expr=\coordindex, y=d1, y error=d1err]{\loctimings};
                        \addplot table [x expr=\coordindex, y=d2, y error=d2err]{\loctimings};
                        \addplot table [x expr=\coordindex, y=d3, y error=d3err]{\loctimings};
                        \addplot table [x expr=\coordindex, y=d4, y error=d4err]{\loctimings};
                        \legend{Dyck-1, Dyck-2, Dyck-3, Dyck-4}
                      \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}[scale=0.35]
      \begin{axis}[
        width=8.3cm,
        height=7cm,
        title={\hspace{-1cm}\textbf{(1b) Latency with unknown locations}},
        ybar,
        bar width=20pt,
        xlabel={Number of errors},
%ylabel={ms to synthesize 1 repair},
        xtick=data,
        axis x line*=bottom,
        axis y line*=left,
        enlarge x limits={abs=0.5},
        ymode=log,
        ytick pos=left,
        xticklabels from table={\unloctimings}{errors},
        ymajorgrids,
        legend pos=north west,
        error bars/y dir=both,
        error bars/y explicit
      ]
        \addplot table [x expr=\coordindex, y=d1]{\unloctimings};
        \addplot table [x expr=\coordindex, y=d2]{\unloctimings};
        \addplot table [x expr=\coordindex, y=d3]{\unloctimings};
        \legend{Dyck-1, Dyck-2, Dyck-3}
      \end{axis}
    \end{tikzpicture}
    \hspace{20pt}
    \begin{tikzpicture}[scale=0.35]
      \begin{axis}[
      width=8.3cm,
      height=7cm,
%    title={\hspace{-1cm}\textbf{Java Brackets}},
      ybar,
      bar width=10pt,
      xlabel={$|\Sigma^*|$},
      ylabel={Top-1 parser acceptance},
      title={\textbf{(2a) Synthetic Java bracket error correction}},
      xtick=data,
      axis x line*=bottom,
      axis y line*=left,
      enlarge x limits={abs=0.5},
      ytick pos=left,
      xticklabels from table={\syntheticerrors}{len},
      ymajorgrids,
      legend pos=north east,
      legend columns=3,
      error bars/y dir=both,
      error bars/y explicit
      ]
      \addplot table [x expr=\coordindex, y=10s]{\syntheticerrors};
      \addplot table [x expr=\coordindex, y=30s]{\syntheticerrors};
      \addplot table [x expr=\coordindex, y=60s]{\syntheticerrors};
      \legend{10s, 30s, 60s}
      \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}[scale=0.35]
      \begin{axis}[
      width=8.3cm,
      height=7cm,
      title={\textbf{(2b) Organic Python bracket error correction}},
      ybar,
      bar width=10pt,
      xlabel={$|\Sigma^*|$},
%ylabel={Parser acceptance},
      xtick=data,
      axis x line*=bottom,
      axis y line*=left,
      enlarge x limits={abs=0.5},
      ytick pos=left,
      xticklabels from table={\naturalerrors}{len},
      ymajorgrids,
      y tick label style={/pgf/number format/.cd,%
      scaled y ticks = false,
      set thousands separator={},
      fixed},
      legend pos=north east,
      legend columns=3,
      error bars/y dir=both,
      error bars/y explicit
      ]
      \addplot table [x expr=\coordindex, y=10s]{\naturalerrors};
      \addplot table [x expr=\coordindex, y=30s]{\naturalerrors};
      \addplot table [x expr=\coordindex, y=60s]{\naturalerrors};
      \legend{10s, 30s, 60s}
      \end{axis}
    \end{tikzpicture}
    \caption{Benchmarking bracket correction latency and accuracy across two bracketing languages, one generated from Dyck-n, and the second uses an abstracted source code snippet with imbalanced parentheses.}
  \end{figure}

  In the second set of experiments, we analyze bracketing errors in a dataset of Java and Python code snippets mined from open-source repositories on GitHub using the Dyck-nw\footnote{Using the Dyck-n grammar augmented with a single additional production, \texttt{Dyck-1} {\color{blue}\texttt{->}} \texttt{w} {\color{blue}\texttt{|}} \texttt{Dyck-1}. Contiguous non-bracket characters are substituted with a single placeholder token, \texttt{w}, and restored verbatim after bracket repair.}, in which all source code tokens except brackets are replaced with a \texttt{w} token. For Java (2a), we sample valid single-line statements with bracket nesting more than two levels deep, synthetically delete one bracket uniformly at random, and repair using Tidyparse, then take the top-1 repair after $t$ seconds, and validate using ANTLR's Java 8 parser. For Python (2b), we sample invalid code fragments uniformly from the imbalanced bracket category of the Break-It-Fix-It (BIFI) dataset~\cite{yasunaga2021break}, a dataset of organic Python errors, which we repair using Tidyparse, take the top-1 repair after $t$ seconds, and validate repairs using Python's \texttt{ast.parse()} method. Since the Java and Python datasets do not have a ground-truth human fix, we report the percentage of repairs that are accepted by the language's official parser for repairs generated under a fixed time cutoff. Although the Java and Python datasets are not directly comparable, we observe that Tidyparse can detect and repair a significant fraction of bracket errors in both languages with a relatively unsophisticated grammar.

%\subsection{Ranking}
%
%Since the number of solutions can be very large, we can use a language model to rank the results maximizing likelihood, or minimizing perplexity, subject to the constraints. This ranking can be used to guide the propagation, sample the choice function, sample hole locations or as a post-processing step after a fixed timeout has expired.

%Alternatively, this expression can be rewritten as a polynomial over GF(2):
%
%\[
%  (v_1 \times w_2 + y_3 + 1) \Leftrightarrow [S \in Y] \Leftrightarrow [Q R \in L(G)]
%\]

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
%  \bibliographystyle{splncs04}
  \bibliography{acmart}
\end{document}